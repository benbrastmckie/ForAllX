%!TEX root = forallx-ubc.tex
\chapter{Natural Deduction Proofs in SL}
\label{ch.ND.proofs}

%JRH to-do: add in some \ellipsesline commands in the proofs 
%take out stuff in strategy about rules of replacement! not legal in SND! 

This chapter introduces a different proof system for Sentential Logic (SL) than the tree method.
The tree method has advantages and disadvantages.
One advantage of trees is that, for the most part, they can be produced in a purely mechanical way where no flash of insight is necessary.
Another advantage is that producing a complete open tree provides a recipe for constructing an interpretation that satisfies the root.
A disadvantage of the tree method is that trees do not provide an intuitive line of reasoning from the premises to the conclusion. 
Although a closed tree shows that the premises together with the negation of the conclusion leads to a contradiction, we don't learn what it is about the premises which lead to the conclusion.
Remember that logic is the study of logically valid arguments, and it would be nice if our proof system gave us a way to argue from the premises to the conclusion in logically valid ways that nevertheless resemble natural forms of reasoning.

Providing such a system of \define{natural deduction} (SD) for SL is the aim of the present chapter.
By contrast with the tree method, SD is intended to model human reasoning, illustrating the connections between various claims.
Consequently, working through a natural deduction proof requires a bit more insight than a tree proof does.
Although natural deduction proofs can be used to prove that an argument is valid, natural deduction system will not tell you if an argument is invalid, nor will it produce an interpretation that invalidates the argument.
That is, there is no equivalent to a completed open tree in natural deduction.

Despite these disadvantages, there is good reason to care about natural deduction systems. % independent of what metalogical properties they may be shown to have.
Recall that the only reason we provided for caring about SL tree proofs is that the tree proof system was shown to be sound and complete.
Accordingly, tree proofs can be used to determine something that we already care about, i.e., which SL arguments are logically valid.
By contrast, we may argue that the natural deduction rules included in SD are forced upon us by the meanings of the connectives included in SL.
For instance, given that $A\eand B$, one may deduce $A$ since what it is for a conjunction like $A\eand B$ to be true is for both of its conjuncts to be true, and so in particular we may conclude that $A$ is true. 
Given a compelling range of inference rules for each of the connectives in SL, one may ask whether it is possible to argue from the premises to the conclusion.
This is to ask whether an argument is \textit{deductively valid}, where knowing the answer to such a question holds interest independent of what metalogical properties that the natural deduction system in question may have.

It will turn out that there is a natural deduction derivation in SD corresponding to every valid SL argument.
Put otherwise, SD is, like the tree method, complete.
Moreover, there are no derivations in SD of a conclusion that fails to be entailed by its premises, and so SD is also sound.
Nevertheless, when an argument is invalid, there is no way to use SD to see that it is valid.
Instead, are left trying different ways to derive the conclusion from the premises, though in the case of an invalid argument, no such attempt will succeed.

In what follows, we will introduce ten basic derivation rules for our five logical connectives in SL.
Each connective will have an introduction and an elimination rule where, taken together, these rules may be taken to capture a certain dimension of the \textit{meaning} of that connective.
In particular, the introduction and elimination rules describe how to reason with each connective.
There is also a trivial rule which lets you reiterate an earlier line in the proof.
This corresponds to the idea that we can always conclude $A$ if $A$ is true.
However trivial, such an inference rule will turn out to have an important role to play in our proof system.

The following section will present the basic rules of inference included in SD.
Given these rules, we will be in a position to provide a precise definition of a proof in SD.
In contrast to trees, SD proofs are sequences of sentences satisfying certain properties.
Already there is something much more natural about sequences of sentences in place of trees; after all, natural reasoning takes place in time, and time is linear.



% \section{Natural Deduction}
%
% The basic idea of a natural deduction proof is simple.
% You begin by writing down the premises that you are arguing from, numbering each in turn.
% We will then make use of the eleven rules alluded to above.
% If you can use the rules to derive a sentence from what you have already written down, then you can add this derived sentence on a new numbered line.
% If, following the rules, you manage to derive the conclusion from the premises, then you've shown that the argument from the premises to the conclusion is \textit{deductively valid}.
%
% We will provide a precise definition of a proof in SD once we have introduced the basic rules of inference.
% However, to get a sense of things, consider the following SL arguments:
%
% \begin{multicols}{2}
% \emph{Modus Ponens:}
% \begin{earg}
% \item $P \eif Q$
% \item $P$
% \item[\therefore] $Q$
% \end{earg}
%
% \emph{Disjunctive Syllogism:}
% \begin{earg}
% \item $P \eor Q$
% \item $\enot P$
% \item[\therefore] $Q$
% \end{earg}
%
% \end{multicols}
%
% Both are valid; you could confirm this with a four-line truth table.
% Either would demonstrate that there is no interpretation satisfying both premises, while falsifying the conclusion.
% The truth table method does not distinguish between these argument forms; it simply shows that they are both valid.
% There is, however, an interesting and important difference between these two argument forms, gestured at by their labels.
% The first argument, \emph{Modus Ponens}, has a distinctive syntactic form: its premises are a conditional and the antecedent of that conditional, and the conclusion is the consequent; the second has a different form.
%
% The natural deduction method is based on the recognition of particular kinds of valid forms.
% They also correspond reasonably well to familiar forms of informal reasoning.
% If you know a conditional, and you also know its antecedent, it is easy to infer its consequent.
% Imagine being sure that if I ate the chilli, I'll get sick, and also being sure that I ate the chilli.
% You will surely infer that I will get sick.
% \emph{Modus ponens} is the name of this kind of conditional reasoning, and there is a special rule for it in our natural deduction system.

\section{Premises, Assumptions, and Scope}
\label{sec:Scope}

A SD proof begins with a (possibly empty) list of premises, where these will be indicated by writing `PR' on the right.
For instance, consider the following list of premises:
\begin{fitchproof}
  \have{1}{A \eif B} \pr{}
  \have{2}{A} \pr{}
\end{fitchproof}
It is important to take note of the vertical line which abuts the premises included above.
Were we to apply one of our rules (in this case conditional elimination discussed below), we can only appeal to sentences which abut this vertical line, or abut a vertical line to the left.
We will refer to such sentences as \define{live} and falling within the \define{scope} of application of our rules.
In this case, there is only one vertical line where the sentences in the first two lines abut the vertical line, and so both sentences fall within the scope of the rules.
Accordingly, we may apply conditional elimination (discussed in detail below) to derive the following:
\begin{fitchproof}
  \hypo{1}{A \eif B} \pr{}
  \hypo{2}{A} \pr{}
  \have{3}{B} \ce{1,2}
\end{fitchproof}
We have added a horizontal line in order to distinguish the premises from the rest of the derivation.
Since $B$ also abuts the vertical line, it too falls within the scope of any further applications of the rules. 
This breaks down if we are to add an assumption.
\begin{fitchproof}
  \hypo{1}{A \eif B} \pr{}
  \hypo{2}{A} \pr{}
  \have{3}{B} \ce{1,2}
  \open
    \hypo{4}{\enot B} \as{}
  \close
\end{fitchproof}
Assumptions open new subproofs which are indicated by indented vertical lines.
In applying our rules, we are free to appeal to any sentences on this new vertical line, or on any vertical lines to the left of our current vertical line.
For instance, we might add the following lines:
\begin{fitchproof}
  \hypo{1}{A \eif B} \pr{}
  \hypo{2}{A} \pr{}
  \have{3}{B} \ce{1,2}
  \open
    \hypo{4}{\enot B} \as{}
    \have{5}{\enot B \eand A} \ai{4,2}
  \close
  \have{6}{\enot B \eif (\enot B \eand A)} \ci{4-5}
\end{fitchproof}
Line $5$ applies conjunction introduction to lines $4$ and $2$, and line $6$ closes the subproof by using conditional introduction (these rules will be discussed below).
Now suppose that we were to continue our proof.
Whereas line $5$ was able to apply rules to any of the sentences above, this is not true of line $7$.
Rather, only the sentences which abut the vertical line on the left fall within the scope of the rules at line $7$.
In particular, both lines $4$ and $5$ are \define{dead} since they belong to a closed subproof and so fall outside the scope of our rules in line $7$. 

The concept of scope is important to get a hold of before introducing the rules in full.
Although it is tempting to say that any sentences above may be conjoined in a new line, this is not true strictly speaking.
Rather, any sentences within the scope of application of a rule may be conjoined, but sentences outside the scope must be avoided.

% he right-most vertical bar of the line you are appealing to must carry down to the line you are currently working on. If it doesn't, then you aren't in the scope of the claim on that line; it is not a live claim, and hence it would be ILLEGAL to appeal to it. 





\section{Reiteration}
\label{sec:reiteration}
% The very first rule is so breathtakingly obvious that it is surprising we bother with it at all.\footnote{From Calgary \S16.2.}

The first rule was already eluded to above.
Given any sentence that falls within the scope of application, the \emph{reiteration rule} (R) allows you to repeat that sentence on a new line.
\begin{fitchproof}
	\have[$\vdots$]{}{\vdots}
	\have[4]{a1}{A \eand B}
	\have[$\vdots$]{}{\vdots}
	\have[10]{a2}{A \eand B} \by{R}{a1}
\end{fitchproof}
Given that we have written `$A \eand B$' on line $4$, we may repeat this sentence at some later line, e.g., line $10$.
We also add a citation which justifies what we have written.
In this case, we write `R', to indicate that we are using the reiteration rule, and we write `$4$', to indicate that we have applied it to line $4$.
Here is the general expression of the rule:
\factoidbox{
\begin{fitchproof}
	\have[m]{a}{\metaA}
	\have[\ ]{c}{\metaA} \by{R}{a}
\end{fitchproof}}
If $\metaA$ occurs on any line $m$ within the scope of application, we can reiterate $\metaA$, justifying this addition by writing `:$m$ R' to indicate that reiteration was applied to line $m$.
Of course, in an actual proof, the lines are numbered, and so $m$ will take on a numerical value.

Here is an example of three legal applications of rule R followed by an illegal application:

\begin{fitchproof}
		\hypo{r}{P} \pr{}
		\open
			\hypo{l}{\enot Q} \as{for funsies (we'll discuss assumptions later)}
			\have{rl}{\enot Q} \by{R (\textsc{legal})}{l}
			\have{n} {P} \r{r}
			\close
      \have{m} {\enot Q \eif P} \ci{l-n}
      \have{k} {P} \by{R (\textsc{legal})}{r}
    \have{con}{\enot Q}\by{R, (\textsc{illegal})}{l}
	\end{fitchproof}

On the second line, we begin a subproof by assuming $\enot Q$.
You can reiterate $\enot Q$ within the subproof, but not when you leave the subproof.
On line $5$, we have left the subproof, and so line $2$ is no longer in scope.
At line $6$, we can reiterate line $1$ which is in scope, but at line $7$ we cannot reiterate line $2$.
Even if we were to open another subproof, we could not appeal to line $2$.
Rather, the lines of a closed subproof are dead, and cannot be used again. 



\section{Conjunction}

Consider the rule for \textit{conjunction introduction} ($\eand$I):

\factoidbox{
\begin{proof}
	\have[m]{a}\metaA{}
	\have[n]{b}\metaB{}
	\have[\ ]{c}{\metaA{}\eand\metaB{}} \ai{a, b}
\end{proof}}

This rule says that if any SL sentences $\metaA$ and $\metaB$ are live, you may derive their conjunction $\metaA \eand \metaB$, thereby introducing a conjunction into the proof.
It is worth noting that $m$ and $n$ need not be consecutive lines, nor do they need to appear in the order listed.
We require only that each line has been established somewhere in the proof, and that both lines are live.

Whereas conjunction introduction licenses the derivation of a conjunction from any two sentences, conjunction elimination lets us do the opposite.
Given any live conjunction, we may derive either of its conjuncts.
For instance, if $A \eand (P \eor Q)$ is live, we may derive $A$, or we may derive $P \eor Q$, but we must choose which.
If we wish to derive both, then two applications of the rule is required, though the order does not matter.

Here is the \textit{conjunction elimination} ($\eand$E) rule:
\factoidbox{
\begin{proof}
	\have[m]{ab}{\metaA{}\eand\metaB{}}
	\have[\ ]{a}\metaA{} \ae{ab}
	\have[\ ]{b}\metaB{} \ae{ab}
\end{proof}}
This rule lets you derive either conjunct.
Although you do not have to derive both conjuncts, you can with two applications of the rule.
Nevertheless, this is a perfectly acceptable proof:

\begin{proof}
	\hypo[1]{ab}{A \eand B} \pr{}
	\have[2]{b}B \ae{ab}
\end{proof}

Note that the {\eand}E rule only requires one sentence, so we write one line number in the justification.
Here is an example illustrating our two conjunction rules working together.
\begin{earg}
\item[] $[(A\eor B)\eif(C\eor D)] \eand [(E \eor F) \eif (G\eor H)]$
\item[\therefore] $[(E \eor F) \eif (G\eor H)] \eand [(A\eor B)\eif(C\eor D)]$
\end{earg}
The main logical operator in both the premise and conclusion is conjunction.
Since conjunction is symmetric, the argument is obviously valid since the two conjunctions have the same two conjuncts in the opposite order.
In order to provide a proof, we begin by writing down the premise on a numbered line indicating that it is a premise with PR.
After the premises, we draw a horizontal line where everything below this line must be justified by a proof rule.
So the beginning of the proof looks like this:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}} \pr{}
\end{proof}
From the premise, we can separate the conjuncts with {\eand}E.
This yields the following proof.
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}} \pr{}
	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
\end{proof}
The {\eand}I rule requires that each of the conjuncts is live somewhere in the proof, though their order and distance from each other does not matter.
Applying the {\eand}I rule to lines $3$ and $2$, we arrive at the desired conclusion.

\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}} \pr{}

	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
	\have{ba}{{[}(E \eor F) \eif (G\eor H){]} \eand {[}(A\eor B)\eif(C\eor D){]}} \ai{b,a}
\end{proof}

This proof may not look terribly interesting or surprising, but it shows how we can use the proof rules together to demonstrate the validity of an argument.
Note that using a truth table to show that this argument is valid would have required a staggering 256 lines, since there are eight sentence letters in the argument.
A proof via trees would be less unwieldy than that, but would not have been as simple or natural of an argument.




\section{Disjunction}

Suppose Ludwig is reactionary.\footnote{This section has been adapted from the Calgary remix \S16.7.}
Then Ludwig is either reactionary or libertarian.
Trivial as this may seem, it speaks to the logic of disjunction.
Just as we may derive either conjunct from a conjunction, we may derive a disjunction from either of its disjuncts.

Thus the \textit{disjunction introduction} ($\eor$I) rule may be stated as follows:
\factoidbox{\begin{fitchproof}
	\have[m]{a}{\metaA}
	\have[\ ]{ab}{\metaA\eor\metaB}\oi{a}
	\have[\ ]{ba}{\metaB\eor\metaA}\oi{a}
\end{fitchproof}}
Since $\metaB$ can be \emph{any} sentence whatsoever, the following is a perfectly acceptable proof:
\begin{fitchproof}
	\hypo{m}{M} \pr{}
	\have{mmm}{M \eor ([(A\eiff B) \eif (C \eand D)] \eiff [E \eand F])}\oi{m}
\end{fitchproof}
Using a truth table to show this would have taken 128 lines.

The disjunction elimination rule is slightly trickier.
Suppose that either Ludwig is reactionary or he is libertarian.
It does not follow that Ludwig is reactionary, for he might be a libertarian.
Equally, we cannot conclude that Ludwig is libertarian, since he might be reactionary.
Since we don't know which disjunct is true, there is nothing that we can deduce from disjunction on its own.
It is for this reason that disjunctions are hard to work with.

Suppose that we could show that if Ludwig's is reactionary, then he is an Austrian economist.
Suppose that we could also show that if Ludwig's is a libertarian, then he is also an Austrian economist.
Even though we don't know whether Ludwig is reactionary or a libertarian, it doesn't matter: in either case he is an Austrian economist.
This is a natural way to make use of a disjunction even when we don't know which disjunct is true.
Generalizing on the line of reasoning above, consider the following \textit{disjunction elimination} ($\eor$E) rule:
\factoidbox{
	\begin{fitchproof}
		\have[m]{ab}{\metaA\eor\metaB}
		\open
			\hypo[i]{a}{\metaA} {} \as{for \eor E}
			\have[\vdots]{d1}{\vdots}
			\have[j]{c1}{\metaC}
		\close
		\open
			\hypo[k]{b}{\metaB}{} \as{for \eor E}
			\have[\vdots]{d2}{\vdots}
			\have[l]{c2}{\metaC}
		\close
		\have[ ]{c}{\metaC}\oe{ab, a-c1,b-c2}
	\end{fitchproof}}
This rule is somewhat clunkier to write down than our previous rules, but the point is fairly simple.
Suppose we have some disjunction, $\metaA \eor \metaB$.
Suppose we have two subproofs, showing us that $\metaC$ follows from the assumption that $\metaA$, and that $\metaC$ also follows from the assumption that $\metaB$.
Then we can infer $\metaC$ from the original disjunction $\metaA \eor \metaB$ together with our two subproofs.
As usual, there can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$.
Moreover, the subproofs and the disjunction can come in any order, and do not have to be adjacent to each other as above.

Some examples might help illustrate this.
Consider this argument:
$$(P \eand Q) \eor (P \eand R) \; \therefore \; P$$
An example proof might run like this:
	\begin{fitchproof}
		\hypo{prem}{(P \eand Q) \eor (P \eand R) } \pr{}
			\open
				\hypo{pq}{P \eand Q} \as{for \eor E}
				\have{p1}{P}\ae{pq}
			\close
			\open
				\hypo{pr}{P \eand R} \as{for \eor E}
				\have{p2}{P}\ae{pr}
			\close
		\have{con}{P}\oe{prem, pq-p1, pr-p2}
	\end{fitchproof}
Here is a slightly harder example.
Consider the following argument:
	$$ A \eand (B \eor C) \therefore (A \eand B) \eor (A \eand C)$$
We may then construct the following proof:
	\begin{fitchproof}
		\hypo{aboc}{A \eand (B \eor C)} \pr{}
		\have{a}{A}\ae{aboc}
		\have{boc}{B \eor C}\ae{aboc}
		\open
			\hypo{b}{B} \as{for \eor E}
			\have{ab}{A \eand B}\ai{a,b}
			\have{abo}{(A \eand B) \eor (A \eand C)}\oi{ab}
		\close
		\open
			\hypo{c}{C} \as{for \eor E}
			\have{ac}{A \eand C}\ai{a,c}
			\have{aco}{(A \eand B) \eor (A \eand C)}\oi{ac}
		\close
	\have{con}{(A \eand B) \eor (A \eand C)}\oe{boc, b-abo, c-aco}
	\end{fitchproof}
As natural as the rules may seem in isolation, it is not always obvious how to put them together to get from some premises to a conclusion.
The ability to construct SD proofs takes some practice, and we'll cover some strategies for finding proofs at the end of the chapter.
Nevertheless, once a natural deduction proof has been constructed, each step is easy to justify, making the derivation in total impervious to doubts.
Moreover, this certainty does not stem from any semantic considerations.
Rather, the proof rules are directly justified by our intuitive understanding of how to use the logical connectives in our language.




\section{Conditional Introduction}

The rule for conditional introduction has already been deployed in the informal proofs in the previous chapter, and should have felt both compelling an familiar.
Here is an abbreviated version of the reasoning which showed up in the completeness proof:
	\begin{quote}
		Assume $\Gamma \nproves \bot$.
    Given the lemmas we have proven, it follows from this assumption that $\Gamma \nmodels \bot$.
    Although we don't know whether $\Gamma \nmodels \bot$ holds independent of our assumption, we may conclude that \textit{if} $\Gamma \nproves \bot$, \textit{then} $\Gamma \nmodels \bot$. 
	\end{quote}
This final step in the argument above is sometimes called \textit{discharging the assumption}.
The ideas is that you help yourself to something that you may not know is true, do some reasoning to arrive at some further claim, then \textit{discharge} your assumption by asserting a conditional claim: if our assumption is true, then the further claim is true.

Here is a somewhat simpler version of the same style of reasoning.
	\begin{quote}
		Ludwig is reactionary. Therefore if Ludwig is libertarian, then Ludwig is both reactionary \emph{and} libertarian.
	\end{quote}
We may regiment this argument as a natural deduction proof by started with one premise $R$ for `Ludwig is reactionary':
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
	\end{fitchproof}
We may now make an additional assumption $L$ for `Ludwig is libertarian'.
In common parlance, we might say something like, ``suppose for the sake of argument,'' or in writing informal proofs, we might say, ``assume for conditional proof.''
However, in our proof system SD, we will indicate that we are adding an assumption by writing `AS' on the right, where it is often helpful to also include `for $\eif$Intro' as a note to your reader.
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
		\open
			\hypo{l}{L} \as{for \eif Intro}{}
	\end{fitchproof}

Note that we are not claiming to have proved $L$ from line 1.
Accordingly, we do not write any justification for the additional assumption on line 2.
Rather, we have started a new subproof by indenting the sentence $L$ and starting a new vertical line.
We have also underlined $L$ since it is playing a role analogous to a premise in our new subproof. 

With this extra assumption in place, we are in a position to use $\eand$I from before.
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
		\open
			\hypo{l}{L} \as{for \eif Intro}{}
			\have{rl}{R \eand L}\ai{r, l}
%			\close
%		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{fitchproof}
Given the assumption $L$, we have deduced $R \eand L$.
The next move to make is to discharge our assumption, or to use other language, to introduce a conditional.
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
		\open
			\hypo{l}{L} \as{for \eif Intro}{}
			\have{rl}{R \eand L}\ai{r, l}
			\close
		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{fitchproof}
Whereas the indented subproof carries out reasoning under the assumption of $L$, line $4$ reverts back to our original proof which carries out reasoning under the assumption of our single premise $R$.
Accordingly, we cannot conclude $R\eand L$ merely under the assumption of $R$ by writing $R\eand L$ at the original level of indenting.
Nevertheless, we cannot assert the conditional $L\eif(R\eand L)$ as given in $4$, where writing this line both discharges the assumption in $2$ and closes the subproof. 
In order to justify line $4$, we list the lines which make up the subproof by putting a hyphen between the first line of the subproof and the last line of the subproof.
In this case there are only two lines, but in general there may be many more.

Generalising on this pattern of reasoning, consider the \textit{conditional introduction} rule:
\factoidbox{
	\begin{fitchproof}
		\open
			\hypo[i]{a}{\metaA} \as{}
			\have[\vdots]{b}{\vdots}
			\have[k]{c}{\metaB}
		\close
		\have[\ ]{ab}{\metaA\eif\metaB}\ci{a-c}
	\end{fitchproof}}



\section{Conditional Elimination}

Many different arguments demonstrate the inference \emph{modus ponens}:

\begin{multicols}{3}
\begin{earg}
\item[] $P \eif \enot Q$
\item[] $P$
\item[\therefore] $\enot Q$
\end{earg}

\begin{earg}
\item[] $\enot P \eif (A \eiff B)$
\item[] $\enot P$
\item[\therefore] $A \eiff B$
\end{earg}

\begin{earg}
\item[] $(P \eor Q) \eif A$
\item[] $P \eor Q$
\item[\therefore] $A$
\end{earg}

\end{multicols}

The natural deduction system of this chapter will include a rule of inference corresponding to \emph{modus ponens} which goes by the name \textit{conditional elimination} ($\eif$E).
Here is the rule:
\factoidbox{
\begin{proof}
	\have[m]{ab}{\metaA{}\eif\metaB{}}
	\have[n]{a}\metaA{}
	\have[\ ]{b}\metaB{} \ce{ab,a}
\end{proof}}
What this rule says is that if you have a conditional $\metaA\eif\metaB$ on line number $m$, and you also have the antecedent of that conditional $\metaA$ on a line $n$, you can write the consequent $\metaB$ on a new line.
In order to justify this inference, we will list the line numbers $m$ and $n$ as well as `$\eif$E' to specify the rule that we are using.
With this rule, we can prove that the arguments given above are valid.
Here are proofs of two of them:


\begin{multicols}{2}

\begin{proof}
	\hypo{if}{P \eif \enot Q} \pr{}
	\hypo{a}P \pr{}
	\have{c}{\enot Q} \ce{if,a}
\end{proof}


\begin{proof}
	\hypo{if}{(P \eor Q) \eif A} \pr{}
	\hypo{a}{P \eor Q} \pr{}
	\have{c}A \ce{if,a}
\end{proof}

\end{multicols}

Notice that these two proofs share the same structure.
We start by listing the premises followed by a horizontal line, where subsequent lines will need to be derived with the rules.
We then apply the conditional elimination rule to get the conclusion, citing the appropriate lines.
One can produce more complicated proofs with the same rule.

\begin{earg}

\item[] $A$ 
\item[] $A \eif B$ 
\item[] $B \eif C$ 
\item[] $C \eif [\enot P \eiff (Q \eor R)]$ 
\item[\therefore] $\enot P \eiff (Q \eor R)$
\end{earg}

We begin by writing our four premises on numbered lines:

\begin{proof}
	\hypo{4}{A} \pr{}
	\hypo{1}{A \eif B} \pr{}
	\hypo{2}{B\eif C} \pr{}
	\hypo{3}{C \eif [\enot P \eiff (Q \eor R)]} \pr{(Want \enot P \eiff (Q \eor R))}
%	\have{b} {B} \ce{1,4}
%	\have{c}{C} \ce{2,b}
%	\have{}{\enot P \eiff (Q \eor R))} \ce{3,c}
\end{proof}

The parenthetical off to the right is optional, but can help to keep track of the conclusion that we are attempting to establish.
The proof will be complete once we derive $\enot P \eiff (Q \eor R)$ by applying the rules to the premises or subsequent lines that result from doing so.
Since we cannot use conditional elimination to get to our desired conclusion directly from our premises, it is worth considering what we can do.
For instance, we can use conditional elimination on lines $1$ and $2$ to get $B$ on a new line, and then repeat using our new line together with line $3$ to get $C$ on yet another new line. 
Continuing in this manner gives us the following:

\begin{proof}
	\hypo{4}{A} \pr{}
	\hypo{1}{A \eif B} \pr{}
	\hypo{2}{B\eif C} \pr{}
	\hypo{3}{C \eif [\enot P \eiff (Q \eor R)]} \pr{want \enot P \eiff (Q \eor R)}
	\have{b} {B} \ce{4,1}
	\have{c}{C} \ce{2,b}
	\have{}{\enot P \eiff (Q \eor R))} \ce{3,c}
\end{proof}

It is important to observe that every line after the premises includes a justification starting with a colon `:' which is followed by the relevant line numbers and the derivation rule in question, in that order.
Once you have derived something from the premises, that new line is available to help justify future lines.
For instance, once we derive line $5$, we were able to use it with line $3$ to derive line $6$.

In order to see the conditional introduction and elimination rules work together, consider:
	$$P \eif Q,\ Q \eif R\ \therefore \ P \eif R$$
We start by listing the premises--- this much is automatic requiring no thinking whatsoever.
But now we have to think about where we are going, i.e., we want to conclude with the conditional $P\eif R$.
A great way to do this is by conditional introduction and so, to use this rule, we must begin by assuming the antecedent $P$ of the conditional we want to conclude.
\begin{fitchproof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P} \as{for \eif I}{}
	\close
\end{fitchproof}
Note that we have made $P$ available, by treating it as an additional assumption.
Having done so, we can use conditional elimination on the first premise to get $Q$.
We can then use conditional elimination on the second premise to get $R$.
Finally, we may discharge $P$ to complete the proof. 
Putting all of this together, we have the following:
\label{HSproof}
\begin{fitchproof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P} \as{for \eif I}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{fitchproof}

% Here is another simple example of an argument form whose validity we can prove using the conditional rules.
%
%
% \begin{earg}
% \item[] $P \eif Q$
% \item[] $Q \eif R$
% \item[\therefore] $P \eif R$
% \end{earg}
% We begin the proof by writing the two premises as assumptions. Since the main logical operator in the conclusion is a conditional, we can expect to use the {\eif}I rule. For that, we need a subproof --- so we write in the antecedent of the conditional as assumption of a subproof. We make a note that we are aiming for the consequent of that conditional:
%
% \begin{proof}
% 	\hypo{pq}{P \eif Q} \pr{}
% 	\hypo{qr}{Q \eif R} \pr{}
% 	\open
% 		\hypo{p}{P} \as{for \eif I, want R}
% 	\close
% \end{proof}
%
% We made $P$ available by assuming it in a subproof, allowing us to use {\eif}E on the first premise. This gives us $Q$, which allows us to use {\eif}E on the second premise. Having derived $R$, we close the subproof. By assuming $P$ we were able to prove $R$, so we apply the {\eif}I rule and finish the proof:
% \phantomsection\label{HSproof}
% \begin{proof}
% 	\hypo{pq}{P \eif Q} \pr{}
% 	\hypo{qr}{Q \eif R} \pr{}
% 	\open
% 		\hypo{p}{P} \as{for \eif I, want R}
% 		\have{q}{Q}\ce{pq,p}
% 		\have{r}{R}\ce{qr,q}
% 	\close
% 	\have{pr}{P \eif R}\ci{p-r}
% \end{proof}

So far the examples have been simple, but perhaps you can already begin to see the strategy that natural deduction proofs sometimes require.
To get good at these, there is no way to avoid putting in some practice.
The good news is that natural deduction proofs are a lot more interesting to construct than tree proofs, and a lot more beneficial: getting good at natural deduction will streamline your reasoning well beyond this course.



\section{The Biconditional}
The rules for the biconditional will be like double-barrelled versions of the rules for the conditional.\footnote{Section from \S16.6 of Calgary} 

In order to prove `$F \eiff G$', for instance, you must be able to prove `$G$' on the assumption `$F$' \emph{and} prove `$F$' on the assumption `$G$'. The biconditional introduction rule ({\eiff}I) therefore requires two subproofs. Schematically, the rule works like this: 
\factoidbox{
\begin{fitchproof}
	\open
		\hypo[i]{a1}{\metaA} \as{for \eiff I}
			\ellipsesline
		\have[j]{b1}{\metaB}
	\close
	\open
		\hypo[k]{b2}{\metaB} \as{for \eiff I}
			\ellipsesline
		\have[l]{a2}{\metaA}
	\close
	\have[\ ]{ab}{\metaA\eiff\metaB}\bi{a1-b1,b2-a2}
\end{fitchproof}}
There can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs can come in any order, and the second subproof does not need to come immediately after the first.

The biconditional elimination rule ({\eiff}E) lets you do a bit more than the conditional rule. If you have the left-hand subsentence of the biconditional, you can obtain the right-hand subsentence. If you have the right-hand subsentence, you can obtain the left-hand subsentence. So we allow:
\factoidbox{
\begin{fitchproof}
	\have[m]{ab}{\metaA\eiff\metaB}
	\have[n]{a}{\metaA}
	\have[\ ]{b}{\metaB} \be{ab,a}
\end{fitchproof}}
and equally:
\factoidbox{\begin{fitchproof}
	\have[m]{ab}{\metaA\eiff\metaB}
	\have[n]{a}{\metaB}
	\have[\ ]{b}{\metaA} \be{ab,a}
\end{fitchproof}}
Note that the biconditional, and the right or left half, can be separated from one another, and they can appear in any order. However, in the citation for $\eiff$E, we always cite the biconditional first.



\section{\emph{Reductio!} (The negation rules)}
Here is a simple mathematical argument in English:
\begin{earg}
\item[] Assume there is some greatest natural number. Call it $A$.
\item[] That number plus one is also a natural number.
\item[] Obviously, $A+1 > A$.
\item[] So there is a natural number greater than $A$.
\item[] This is impossible, since $A$ is assumed to be the greatest natural number.
\item[\therefore] There is no greatest natural number.
\end{earg}

This kind of argument form is traditionally called a \emph{reductio}. Its full Latin name is \emph{reductio ad absurdum}, which means `reduction to absurdity.' (You will also sometimes see proofs of this form described as `indirect' proofs.) In a \emph{reductio}, we assume something for the sake of argument --- for example, that there is a greatest natural number. Then we show that the assumption leads to two contradictory sentences --- for example, that $A$ is the greatest natural number and that it is not. In this way, we show that the original assumption must have been false.

The basic rules for negation will allow for arguments like this. Like the Conditional Introduction rule (\eif I), our negation rules require us to make a new assumption with no justification --- we draw a new vertical line, and a note to ourselves as to what we are trying to do. If we assume something and show that it leads to contradictory sentences, then we have proven the negation of the assumption. This is the Negation Introduction ({\enot}I) rule:

%try the  \ellipsesline command line to insert dots more easily!

\factoidbox{
\begin{proof}
\open
	\hypo[m]{na}\metaA{} \as{for \enot I}
	\have[n]{b}\metaB{}
	\have[o]{nb}{\enot\metaB{}}
\close
\have[\ ]{a}[\ ]{\enot\metaA{}}\ni{na-nb} %note that UBC has a more complex citation convention: {na-b, na-nb}
\end{proof}}

Let's walk through this step-by-step: On line m, we assume \metaA{} for \emph{reductio}. Our goal is then to derive a contradiction, represented by two sentences \metaB{} and \enot \metaB{} on separate lines. Not necessarily in a row; not necessarily in this order. We just need two sentences of the form \metaB{} and \enot \metaB{} to appear SOMEWHERE within the subproof, at the same scope-line level as our starting assumption \metaA{}. Note that \metaB{} could even be the very SAME sentence as \metaA{}, e.g. both could be `P.'. 

Once we have derived this contradictory pair of sentences, we get to pop out of our subproof, discharging the assumption and introducing that big, beautiful negation: \enot \metaA{}. We cite the whole subproof and hence use a HYPHEN!

%The {\enot}I rule discharges the assumption \metaA{} for \emph{reductio}, concluding with its negation \enot\metaA{}, when it's shown that some sentence and its negation each follow from the assumption. It cites two (overlapping) ranges: a subproof from the assumption to some sentence \metaB{}, and a subproof from that same assumption to \enot\metaB{}. 
%We write `for reductio' to the right of the assumption, as a note to ourselves, a reminder of why we started the subproof. It is not formally part of the proof, but it is helpful for thinking clearly it and planning ahead.

To see how the rule works, suppose we want to prove an instance of the law of non-contradiction: $\enot(G \eand \enot G)$. We can prove this without any premises by immediately starting a subproof. We want to apply {\enot}I to the subproof, so we assume $(G \eand \enot G)$. We then get an explicit contradiction by {\eand}E. The proof looks like this:

\begin{proof}
	%\open %commenting this out gets rid of extra scope line
		\hypo{gng}{G\eand \enot G}\as{for reductio}
		\have{g}{G}\ae{gng}
		\have{ng}{\enot G}\ae{gng}
	\close
	\have{ngng}{\enot(G \eand \enot G)}\ni{gng-ng}
\end{proof}

{\color{black}This is our first example of a natural deduction proof with no premises: notice that there's NO scopeline to the left of the conclusion. It's chillin' outside the box. This syntactical feature encodes the fact that our conclusion is a tautology (although to really conclude this, we would need to first show that our natural deduction system is SOUND---see Section~\ref{sec:soundness}. We will prove the soundness of SD later in the semester, using a supplemental chapter not included here). Also note that less-dramatic \textit{Carnap} always includes an outer line anyways!}

The Negation Elimination ({\enot}E) rule works in much the same way. If we assume \enot\metaA{} and show that it leads to a sentence and its negation, we have effectively proven \metaA{} {\color{black}(presupposing that the law of excluded middle holds!)}. So the rule looks like this:

\factoidbox{
\begin{proof}
\open
	\hypo[m]{na}{\enot\metaA{}}\as{for \enot E}{}
	\have[n]{b}\metaB{}
	\have[o]{nb}{\enot\metaB{}}
\close
\have[\ ]{a}[\ ]\metaA{}\ne{na-nb}
\end{proof}}

Below is an example proof using Reiteration to help with a Negation Introduction:

\begin{proof}
	\hypo{p}{P} \pr{}
	\hypo{qnp}{Q \eif \enot P} \pr{want \enot Q}
	\open
		\hypo{q}{Q} \as{for reductio}{}
		\have{np}{\enot P}\ce{qnp,q}
		\have{nnp}{P}\by{R}{p}
	\close
	\have{nq}{\enot Q}\ni{q-nnp}
\end{proof}

Negation Introduction requires that one show that some sentence and its negation are both derivable from the assumption for \emph{reductio}. In this case, we establish that $P$ and $\enot P$ both follow from the assumption that $Q$. In the case of $\enot P$, we can get it directly via Conditional Elimination; the easiest way to get the $P$ that we need is simply to reiterate it from line 1.

\textit{Whew}! Pat yourself on the back---you've now gotten through the 10+1 basic rules (two for each of the five connectives, plus reiteration). Take a look at the glossary page to soak them all in! 

(An aside: Negation Elimination is not a legal rule in all logical systems. What we are really doing is first applying Negation Introduction, arriving at \enot \enot \metaA{}, and then applying `double negation elimination', namely the classical truth that \enot \enot \metaA{} entails \metaA{}. But in some non-classical logics, Double Negation Elimination is itself illegal. As in life, it's easier to make things negative than to take a sad song and make it better.)
%turn something negative into something positive.) 





\section{Exact matches (`oh to follow a rule!')}

{\color{black}You might think that following a rule is easy. You are wrong!!! This section describes one of the most common mistakes people make. The fact that we are liable to make this kind of mistake shows just how hard it is to follow a rule.} 

Conditional Elimination, as well as all of our other natural deduction rules, are syntactically defined. That is to say, the application of the rules depends on the exact shape of the SL sentences in question. Here, again, is the formal statement of the rule:

\begin{proof}
	\have[m]{ab}{\metaA{}\eif\metaB{}}
	\have[n]{a}\metaA{}
	\have[\ ]{b}\metaB{} \ce{ab,a}
\end{proof}

It says that any time one has, on one line, a sentence made up of some sentence \metaA{}, followed by the `\eif' symbol, followed by some sentence \metaB{}, where one also has \metaA{} on another line, one may derive \metaB{}. This is the only pattern of inference that this rule permits. \metaA{} and \metaB{} can be any sentences of SL, but a line justified by Conditional Elimination must fit this pattern exactly. It is not enough that one can `just see' that a given sentence follows via a similar pattern of inference.

For example, this is \emph{not} a legal derivation in our system:

\begin{proof}
	\hypo{ab}{P \eif (A \eand B)} \pr{}
	\hypo{a}{P} \pr{}
	\have{b}{B}\by{ILLEGAL \eif E}{ab,a}
\end{proof}

The Conditional Elimination rule requires that the new sentence derived be the consequent of the conditional cited. But in this example, $B$ is not the consequent of $P\eif (A \eand B)$ --- $A \eand B$ is. It is true that $B$ obviously follows from $A \eand B$, but the Conditional Elimination rule doesn't allow you to derive things just because they obviously follow. (Neither does any other rule in our formal system.) To derive $B$ from these premises we'll need to use another rule. (In particular, we will want to use the Conjunction Elimination rule, given below.)

To check to make sure you are applying the rules correctly, one good heuristic is to think about whether you are relying on the rule itself, or on your intuitive understanding of the meanings of the symbols we use in SL. Your intuitive understanding is a good way to think about which rules to use, but to check to make sure you're using the rules properly, think about whether the rules' exact formulations could explain why it is permissible to extend the derivation in the exact way you're working with. Pretend, for instance, that you have no idea what the `\eif' symbol means, but you do know that if you have two sentences joined by it on one line, and the first of those two sentences on another line, then you are allowed to copy down the second sentence exactly on a new line. This --- and no more --- is what the Conditional Elimination rule permits you to do. (This is what we mean when we say the rule is syntactically defined.) It would be pretty trivial to write a computer program to check to see whether a line is properly derived via Conditional Elimination (unlike us, computers are VERY good at following rules, since computers are defined via rules).


\section{Additional assumptions and subproofs}
The rule $\eif$I invoked the idea of making additional assumptions. These need to be handled with some care. Consider this proof:
\begin{fitchproof}
	\hypo{a}{A} \pr{}
	\open
		\hypo{b1}{B} \as{for \eif I}
		\have{b2}{B} \by{R}{b1}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
\end{fitchproof}
This is perfectly in keeping with the rules we have laid down already, and it should not seem particularly strange. Since `$B \eif B$' is a tautology, no particular premises should be required to prove it.

But suppose we now tried to continue the proof as follows:
\begin{fitchproof}
	\hypo{a}{A} \pr{}
	\open
		\hypo{b1}{B} \as{for \eif I}
		\have{b2}{B} \by{R}{b1}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
	\have{b}{B} \by{ILLEGAL attempt to invoke $\eif$E on lines 3 and 4}{b2,con}
	%\have [\ ]{x}{} 
\end{fitchproof}
%\by{to invoke $\eif$E}{con, b2}
If we were allowed to do this, it would be a disaster. It would allow us to prove any sentence letter from any other sentence letter. However, if you tell me that Anne is fast (symbolized by `$A$'), we shouldn't be able to conclude that Queen Boudica stood twenty-feet tall (symbolized by `$B$')! We must be prohibited from doing this, but how are we to implement the prohibition?

We can describe the process of making an additional assumption as one of performing a \emph{subproof}: a subsidiary proof within the main proof. When we start a subproof, we draw another vertical line to indicate that we are no longer in the main proof. Then we write in the assumption upon which the subproof will be based. A subproof can be thought of as essentially posing this question: \emph{what could we show, if we also make this additional assumption?} (Compare: What could we achieve, if we only believed in ourselves!?)

When we are working within the subproof, we can refer to the additional assumption that we made in introducing the subproof, and to anything that we obtained from our original assumptions. (After all, those original assumptions are still in effect.) At some point though, we will want to stop working with the additional assumption: we will want to return from the subproof to the main proof. To indicate that we have returned to the main proof, the vertical line for the subproof comes to an end. At this point, we say that the subproof is \define{closed}. Having closed a subproof, we have set aside---i.e. `discharged'---the additional assumption, so it would be illegitimate to rely on any line that depends on that additional assumption. Thus we stipulate:
\factoidbox{To cite an individual line when applying a rule:
\begin{enumerate}
\item the line must come before the line where the rule is applied, but 
\item not occur within a subproof that has been closed before the line where the rule is applied.
\end{enumerate}}
This stipulation rules out the disastrous attempted proof above. The application of rule $\eif$E on line~$5$ requires that we cite two individual lines from earlier in the proof. One of these lines (namely, line~$3$) occurs within a subproof (lines $2$--$3$). By line $5$, where we have to cite line $3$, the subproof has been closed. This is illegitimate: we are not allowed to cite line $3$ on line $5$.

Closing a subproof is called \define{discharging} the assumptions of that subproof. So we can put the point this way: \emph{you cannot refer back to anything that was obtained using discharged assumptions}.

Subproofs, then, allow us to think about what we could show, if we made additional assumptions. The point to take away from this is not surprising---in the course of a proof, we have to keep very careful track of what assumptions we are making, at any given moment. Our proof system does this very graphically. (Indeed, that's precisely why we have chosen to use \emph{this} proof system).\footnote{You can thank me for not using a Lemmon system that lacks this feature: life is full of lemons; I learned logic on a Lemmon; Lemmon has many virtues; I prefer limes that rhyme with `lines'.}

Once we have started thinking about what we can show by making additional assumptions, nothing stops us from posing the question of what we could show if we were to make \emph{even more} assumptions? This might motivate us to introduce a subproof within a subproof. Here is an example using only the rules which we have considered so far: 
\begin{fitchproof}
\hypo{a}{A} \pr{}
\open
	\hypo{b}{B} \as{for \eif I}
	\open
		\hypo{c}{C} \as{for \eif I}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif (C \eif (A \eand B))}\ci{b-cab}
\end{fitchproof}
Notice that the citation on line~$4$ refers back to the initial assumption (on line 1) and an assumption of a subproof (on line~$2$). This is perfectly in order, since neither assumption has been discharged at the time (i.e., by line~$4$).

Again, though, we need to keep careful track of what we are assuming at any given moment. Suppose we tried to continue the proof as follows:
\begin{fitchproof}
\hypo{a}{A} \pr{}
\open
	\hypo{b}{B} \as{for \eif I}
	\open
		\hypo{c}{C} \as{for \eif I}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif(C \eif (A \eand B))}\ci{b-cab}
\have{bcab}{C \eif (A \eand B)}\by{ILLEGAL attempt to invoke $\eif$I on lines 3-4}{c-ab}
%\have [\ ]{x}{} \by{to invoke $\eif$I}{c-ab}
\end{fitchproof}
This would be awful. If we tell you that Anne is smart, you should not be able to infer that, if Cath is smart (symbolized by `$C$') then \emph{both} Anne is smart and Queen Boudica stood 20-feet tall! But this is just what such a proof would suggest, if it were permissible.

The essential problem is that the subproof that began with the assumption~`$C$' depended crucially on the fact that we had assumed `$B$' on line~$2$. By line~$6$, we have \emph{discharged} the assumption~`$B$': we have stopped asking ourselves what we could show, if we also assumed `$B$'. So it is simply cheating, to try to help ourselves (on line~$7$) to the subproof that began with the assumption~`$C$'. 

Thus we stipulate, much as before, that \textbf{a subproof can only be cited on a line if it does not occur within some other subproof that is already closed at that line}. Perhaps put more simply, \textbf{you can't appeal to a subproof if you are outside the scope of an assumption(s) that the subproof depends on}. The attempted disastrous proof violates this stipulation. The subproof of lines $3$--$4$ occurs within a subproof that ends on line~$5$ (it depends on the assumption on line 2). So it cannot be invoked on line~$7$ (since line 7 is OUTSIDE the scope of the assumption on line 2). 

Here is one further case we have to exclude:
\begin{fitchproof}
\hypo{a}{A} \pr{}
\open
	\hypo{b}{B} \as{for \eif I}
	\open
		\hypo{c}{C} \as{for \eif I}
	\have{bc}{B \eand C}\ai{b,c}
	\have{c2}{C}\ae{bc}
	\close
\close
\have{bcab}{B \eif C}\by{ILLEGAL attempt to invoke $\eif$I on lines 2-5}{b-c2}
%\have [\ ]{x}{} \by{}{b-c2}
\end{fitchproof}
Here we are trying to cite a subproof that begins on line~$2$ and ends on line~$5$---but the sentence on line~$5$ depends not only on the assumption on line~$2$, but also on one another assumption (line~$3$) which we have not discharged at the end of the subproof. The subproof started on line~$3$ is still open at line~$3$. But $\eif$I requires that the last line of the subproof \emph{only} relies on the assumption of the subproof being cited, i.e., the subproof beginning on line~$2$ (and anything before it), and not on assumptions of any subproofs within it. In particular, the last line of the subproof cited must not itself lie within a nested subproof.

\factoidbox{To cite a subproof when applying a rule:
\begin{enumerate} 
\item the cited subproof must come entirely before the application of the rule where it is cited, 
\item the cited subproof must not lie within some other closed subproof which is closed at the line it is cited, and 
\item its last line of the cited subproof must not occur inside a nested subproof.
\end{enumerate}}

One last point to emphasize how rules can be applied: where a rule requires you to cite an individual line, you cannot cite a subproof instead; and where it requires you to cite a subproof, you cannot cite an individual line instead. So for instance, this is incorrect:
\begin{fitchproof}
\hypo{a}{A} \pr{}
\open
	\hypo{b}{B} \as{for \eif I}
	\open
		\hypo{c}{C} \as{for \eif I}
	\have{bc}{B \eand C}\ai{b,c}
	\have{c2}{C}\ae{bc}
	\close
	\have{c3}{C}\by{ILLEGAL attempt to invoke R}{c-c2}
%\have [\ ]{x}{} \by{to invoke R}{c-c2}
\close
\have[7]{bcab}{B \eif C}\ci{b-c3}
\end{fitchproof}
Here, we have tried to justify $C$ on line~$6$ by the reiteration rule, but we have cited the subproof on lines $3$--$5$ with it. That subproof is closed and can in principle be cited on line~$6$. (For instance, we could use it to justify $C \eif C$ by $\eif$I.) But the reiteration rule~R requires you to cite an individual line, so citing the entire subproof is inadmissible (even if that subproof contains the sentence~$C$ we want to reiterate).


It is always permissible to open a subproof with any assumption. However, there is some strategy involved in picking a useful assumption. Starting a subproof with an arbitrary, wacky assumption would just waste lines of the proof. In order to obtain a conditional by {\eif}I, for instance, you must assume the antecedent of the conditional in a subproof.

Equally, it is always permissible to close a subproof (and discharge its assumptions). However, it will not be helpful to do so until you have reached something useful. Once the subproof is closed, you can only cite the entire subproof in any justification. Those rules that call for a subproof or subproofs, in turn, require that the last line of the subproof is a sentence of some form or other. For instance, you are only allowed to cite a subproof for $\eif$I if the line you are justifying is of the form $\metaA \eif \metaB$, $\metaA$ is the assumption of your subproof, and $\metaB$ is the last line of your subproof.


\iffalse %Ichikawa version: 

\subsection{Reiteration}
\label{sec:reiteration}

In addition to the Introduction and Elimination rules for each logical operator, we will also have one more basic rule: Reiteration (R). If you already have shown something in the course of a proof, the Reiteration rule allows you to repeat it on a new line. Put formally:

\begin{proof}
	\have[m]{a1}\metaA{}
	\have[\ ]{a2}\metaA{} \by{R}{a1}
\end{proof}

%Josh: I think the following has a problem anyways, since to apply Modus Tollens, one would need \enot (\enot P), so would have to construct this from two applications of negation introduction anyways.  
%You may have noticed that there is another way to prove $\enot Q$ from these premises --- one may do it in a single step via \emph{modus tollens}. Recall that \emph{modus tollens} is not one of our \emph{basic} rules. It is a \emph{derived} rule that lets us take certain shortcuts. But anything that can be proven with \emph{modus tollens} can be proven with the basic rules listed above. In fact, the proof we have just seen, using Negation Introduction and Reiteration, provides a template. Anything you could get to with one step via \emph{modus tollens}, you could also get to in four steps via Negation Introduction, Conditional Elimination, and Reiteration.

\fi 


\iffalse

\subsection*{Disjunction Elimination from Calgary Appendix}
\begin{fitchproof}
	\have[m]{a}{\metaA}
	\have[\ ]{ab}{\metaA\eor\metaB}\oi{a}
	\have[m]{a}{\metaA}
\\	\have[\ ]{ba}{\metaB\eor\metaA}\oi{a}
	\have[m]{ab}{\metaA\eor\metaB}
\\	\open
		\hypo[i]{a}{\metaA}
		\have[j]{c1}{\metaC}
	\close
	\open
		\hypo[k]{b}{\metaB}
		\have[l]{c2}{\metaC}
	\close
	\have[\ ]{c}{\metaC} \oe{ab,a-c1, b-c2}
\end{fitchproof}

An example from Gordon in Kluwer Fitch:

\begin{equation*}
  \begin{fitch}
    \fh a \supset b & A  \\
    	\fa\fh a & A /$\supset$ I \\
   	 \fa\fa b & ($\supset$E) 1,2 \\
    	\fa\fa    a \&b & ($\&$I) 2,3 \\
	\fa a \supset (a \& b) & ($\supset$I) 2-5
    \end{fitch}
\end{equation*} 


\fi 



















\section{Proof strategy}
\label{sec.SL.ND.strategy}



Like a good meal, there's no `one-size fits all' recipe for proofs. And there is no substitute for practice! Here, though, are some rules of thumb and strategies to keep in mind.


\paragraph{Work backwards from what you want.}
The ultimate goal is to derive the conclusion. Look at the conclusion and ask what the introduction rule is for its main logical operator. This gives you an idea of what should happen \emph{just before} the last line of the proof. Then you can treat this line as if it were your goal. Ask what you could do to derive this new goal.

For example: If your conclusion is a conditional $\metaA{}\eif\metaB{}$, plan to use the {\eif}I rule. This requires starting a subproof in which you assume \metaA{}. In the subproof, you want to derive \metaB{}.

\paragraph{Work forwards from what you have.}
When you are starting a proof, look at the premises; later, look at the sentences that you have derived so far. Think about the elimination rules for the main operators of these sentences. These will tell you what your options are.

For example: If you have a conditional \metaA{}\eif\metaB{}, and you also have \metaA{}, {\eif}E is a pretty natural choice.

For a short proof, you might be able to eliminate the premises and introduce the conclusion. A long proof is formally just a number of short proofs linked together, so you can fill the gap by alternately working back from the conclusion and forward from the premises.


\paragraph{Change what you are looking at.}
Replacement rules can often make your life easier. If a proof seems impossible, try out some different substitutions.

For example: It is often difficult to prove a disjunction using the basic rules. If you want to show $\metaA{}\eor\metaB{}$, it is often easier to show $\enot\metaA{}\eif\metaB{}$ and use the MC rule.

Some replacement rules should become second nature. If you see a negated disjunction, for instance, you should immediately think of DeMorgan's rule.

\paragraph{Do not forget Reductio! (i.e. indirect proof).}

{\color{black}Unlike good meals, there is a one-size fits all approach to \textit{bad} meals: it's called the microwave! And in many of our problems, if you are really stuck, you'll want to give Negation Elimination (or Introduction) a try: assume the negation of what you want to prove. You can often get pretty far with this. It's the logic equivalent of \textit{nuking} that problem. It often works, albeit in a messy way that leaves your proof feeling stodgy. Yum.} 

To repeat: If you cannot find a way to show something directly, try assuming its negation.

Remember that most proofs can be done either indirectly or directly. One way might be easier --- or perhaps one sparks your imagination more than the other --- but either one is formally legitimate.

\paragraph{Repeat as necessary.} Once you have decided how you might be able to get to the conclusion, ask what you might be able to do with the premises. Then consider the target sentences again and ask how you might reach them.

\paragraph{Persist.}
Try different things. If one approach fails, then try something else. {\color{black}(Nevertheless, Logic persisted---Logic ain't ever gonna quit!).}
% 




\section{Proof-theoretic concepts}

As we did in our discussion of trees, we will again use the symbol `$\vdash$' to indicate provability. Provability is relative to a proof system, so the meaning of the `$\vdash$' symbol featured in this chapter should be distinguished from the one we used for trees. When necessary, we can specify the single turnstile with reference to the proof system in question, letting `$\vdash_{T}$' stand for provability in the tree system, and `$\vdash_{ND}$' stand for provability in this natural deduction system. For the most part in this chapter, though, we'll be interested in natural deduction, so unless it is specified otherwise, you can understand `$\vdash$' to mean `$\vdash_{ND}$'.

The {double turnstile} symbol `$\models$', remains unchanged. It stands for semantic entailment, as described in ch.~\ref{ch.SLmodels}.

When we write $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash_{ND}\metaB{}$, this means that it is possible to give a natural deduction derivation of \metaB{} from premises $\metaA{}_1$,$\metaA{}_2$,$\ldots$. With just one premise, we leave out the curly brackets, so $\metaA{}\vdash\metaB{}$ means that there is a proof of \metaB{} with \metaA{} as a premise. Naturally, $\vdash\metaA{}$ means that there is a proof of \metaA{} that has no premises. You can think of it as shorthand for $\emptyset\vdash\metaA{}$. 

For notational completeness, we can understand $\metaSetX{}\vdash\bot$ to mean that from \metaSetX{}, we could prove an arbitrary contradiction. In other words, $\metaSetX{}$ is an inconsistent set.

Logical proofs are sometimes called \emph{derivations}. So $\metaA{}\vdash\metaB{}$ can be read as `\metaB{} is derivable from \metaA{}.'

A \define{theorem} is a sentence that is derivable without any premises; i.e., \metaA{} is a theorem if and only if $\vdash\metaA{}$.

It is not too hard to show that something is a theorem --- you just have to give a proof of it. How could you show that something is \emph{not} a theorem? If its negation is a theorem, then you could provide a proof. For example, it is easy to prove $\enot(P \eand \enot P)$, which shows that $(P \eand \enot P)$ cannot be a theorem. For a sentence that is neither a theorem nor the negation of a theorem, however, there is no easy way to show this. You would have to demonstrate not just that certain proof strategies fail, but that no proof is possible. Even if you fail in trying to prove a sentence in a thousand different ways, perhaps the proof is just too long and complex for you to make out. As we've emphasized already, this is a difference between our natural deduction system and the tree method. (For just the same reason, the natural deduction system doesn't provide a straightforward way to demonstrate that an argument form is invalid.)

Two sentences \metaA{} and \metaB{} are \define{provably equivalent} if and only if each can be derived from the other; i.e., $\metaA{}\vdash\metaB{}$ and $\metaB{}\vdash\metaA{}$.

It is relatively easy to show that two sentences are provably equivalent --- it just requires a pair of proofs. Showing that sentences are \emph{not} provably equivalent would be much harder. It would be just as hard as showing that a sentence is not a theorem. (In fact, these problems are interchangeable. Can you think of a sentence that would be a theorem if and only if \metaA{} and \metaB{} were provably equivalent?)

The set of sentences $\{\metaA{}_1,\metaA{}_2,\ldots\}$ is \define{provably inconsistent} if and only if contradictory sentences are derivable from it; i.e., for some sentence \metaB{}, $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash\metaB{}$ and $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash\enot \metaB{}$. This is equivalent to $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash\bot$.

It is easy to show that a set is provably inconsistent: You just need to assume the sentences in the set and prove a contradiction. Showing that a set is \emph{not} provably inconsistent will be much harder. It would require more than just providing a proof or two; it would require showing that proofs of a certain kind are \emph{impossible}.


\section{Proofs and models}
As you might already suspect, there is a connection between \emph{theorems} and \emph{tautologies}.

There is a formal way of showing that a sentence is a theorem: Prove it. For each line, we can check to see if that line follows by the cited rule. It may be hard to produce a twenty line proof, but it is not so hard to check each line of the proof and confirm that it is legitimate --- and if each line of the proof individually is legitimate, then the whole proof is legitimate. Showing that a sentence is a tautology, though, requires reasoning in English about all possible models. There is no formal way of checking to see if the reasoning is sound. Given a choice between showing that a sentence is a theorem and showing that it is a tautology, it would be easier to show that it is a theorem.

By contrast, there is no formal way of showing that a sentence is \emph{not} a theorem. We would need to reason in English about all possible proofs. Yet there is a formal method for showing that a sentence is not a tautology. We need only construct a model in which the sentence is false. Given a choice between showing that a sentence is not a theorem and showing that it is not a tautology, it would be easier to show that it is not a tautology.

Fortunately, a sentence is a theorem if and only if it is a tautology. If we provide a proof of $\vdash\metaA{}$ and thus show that it is a theorem, it follows that \metaA{} is a tautology; i.e., $\models\metaA{}$. Similarly, if we construct a model in which \metaA{} is false and thus show that it is not a tautology, it follows that \metaA{} is not a theorem.

In general, $\metaA{}\vdash\metaB{}$ if and only if $\metaA{}\models\metaB{}$. As such:
\begin{itemize}
\item An argument is \emph{valid} if and only if \emph{the conclusion is derivable from the premises}.
\item Two sentences are \emph{logically equivalent} if and only if they are \emph{provably equivalent}.
\item A set of sentences is \emph{consistent} if and only if it is \emph{not provably inconsistent}.
\end{itemize}
You can pick and choose when to think in terms of proofs and when to think in terms of models, doing whichever is easier for a given task. Table \ref{table.ProofOrModel} summarizes when it is best to give proofs and when it is best to give models.

In this way, proofs and models give us a versatile toolkit for working with arguments. If we can translate an argument into SL, then we can measure its logical weight in a purely formal way. If it is deductively valid, we can give a formal proof; if it is invalid, we can provide a formal counterexample.

\begin{table}
\begin{center}
\begin{tabular*}{\textwidth}{p{10em}|p{10em}|p{10em}|}
\cline{2-3}

 & \multicolumn{1}{|c|}{YES} & \multicolumn{1}{|c|}{NO}\\
\cline{2-3}

Is \metaA{} a tautology? & prove $\vdash\metaA{}$ & give a model in which \metaA{} is false\\
\cline{2-3}

Is \metaA{} a contradiction? &  prove $\vdash\enot\metaA{}$ & give a model in which \metaA{} is true\\
\cline{2-3}

Is \metaA{} contingent? & give a model in which \metaA{} is true and another in which \metaA{} is false & prove $\vdash\metaA{}$ or $\vdash\enot\metaA{}$\\
\cline{2-3}

Are \metaA{} and \metaB{} equivalent? & prove \mbox{$\metaA{}\vdash\metaB{}$} and \mbox{$\metaB{}\vdash\metaA{}$}  & give a model in which \metaA{} and \metaB{} have different truth values\\
\cline{2-3}

Is the set \model{A} consistent? & give a model in which all the sentences in \model{A} are true & taking the sentences in \model{A}, prove \metaB{} and \enot\metaB{}\\
\cline{2-3}

Is the argument `\metaA{}, \metaB{}, \ldots \mbox{\therefore\ \metaC{}'} valid? & prove $\metaA{}, \metaB{}, \ldots \vdash\metaC{}$ & give a model in which \{\metaA{}, \metaB{}, \ldots \} is satisfied and \metaC{} is falsified\\
\cline{2-3}
\end{tabular*}
\end{center}
\caption{Sometimes it is easier to show something by providing proofs than it is by providing models. Sometimes it is the other way round.  It depends on what you are trying to show.}
\label{table.ProofOrModel}
\end{table}

\FloatBarrier

\section{Soundness and completeness}
\label{sec:soundness}

Chapter \ref{ch.SLsoundcomplete} considered the soundness and completeness of the tree method at length; it proved that this method was both sound ($\metaSetX{}\vdash_{T}\metaA{}$ only if $\metaSetX{}\models{}$\metaA{}) and complete ($\metaSetX{}\models{}$\metaA{} only if $\metaSetX{}\vdash_{T}\metaA{}$). We said in the last section that $\metaA{}\vdash_{ND}\metaB{}$ if and only if $\metaA{}\models\metaB{}$; this is equivalent to saying that our natural deduction system, too, is sound and complete. Given the soundness and completeness of the tree method, this also means that our two proof systems are equivalent in the sense that anything provable in one is also provable in the other ($\metaSetX{}\vdash_{T}\metaA{})$ iff ($\metaSetX{}\vdash_{ND}\metaA{}$).

How can we know that our natural deduction method is sound? A proof system is \define{sound} if there are no derivations corresponding to invalid arguments. Demonstrating that the proof system is sound would require showing that any possible proof in our system is the proof of a valid argument. There is a fairly simple way of approaching this in a step-wise fashion. If using the {\eand}E rule on the last line of a proof could never change a valid argument into an invalid one, then using the rule many times could not make an argument invalid. Similarly, if using the {\eand}E and {\eor}E rules individually on the last line of a proof could never change a valid argument into an invalid one, then using them in combination could not either.

The strategy is to show for every rule of inference that it alone could not make a valid argument into an invalid one. It follows that the rules used in combination would not make a valid argument invalid. Since a proof is just a series of lines, each justified by a rule of inference, this would show that every provable argument is valid.

Consider, for example, the {\eand}I rule. Suppose we use it to add \metaA{}\eand\metaB{} to a valid argument. In order for the rule to apply, \metaA{} and \metaB{} must already be available in the proof. Since the argument so far is valid, \metaA{} and \metaB{} are either premises of the argument or valid consequences of the premises. As such, any model in which the premises are true must be a model in which \metaA{} and \metaB{} are true. According to the definition of \define{truth in SL}, this means that \metaA{}\eand\metaB{} is also true in such a model. Therefore, \metaA{}\eand\metaB{} validly follows from the premises. This means that using the {\eand}I rule to extend a valid proof produces another valid proof.

In order to show that the proof system is sound, we would need to show this for the other inference rules. Since the derived rules are consequences of the basic rules, it would suffice to provide similar arguments for the 16 other basic rules. The reasoning is extremely similar to that given in the soundness proof for trees in the previous chapter. We will not go through it in detail here.

What of completeness? Why think that \emph{every} valid argument is an argument that can be proven in our natural deduction system? That is, why think that $\metaA{}\models\metaB{}$ implies $\metaA{}\vdash\metaB{}$? Our system \emph{is} also complete, but the completeness proof for natural deduction is a bit more complex than the completeness proof for trees. (In the case of trees, we had a mechanical method that was guaranteed to find proofs if they exist; we have seen no such method here, which makes proving these general results harder.) This proof is beyond the scope of this book.

The important point is that, happily, the proof system for SL is both sound and complete. Consequently, we may freely use this natural deduction method to draw conclusions about models in SL.

\section{Bonus stuff that you can skip!}

{\color{black}This section contains some additional material from Ichikawa's version that could be more confusing or overwhelming, at least on a first reading (this chapter is already very long). You won't be allowed to use the following derived rules on \textit{Carnap} or on the problems, so learning them will probably just make you yearn for a logical system with more legal rules. 

They are instructive examples though! And of course you can always write out the long-form proof of each derived rule (so learning them can be handy tricks).} 

\subsection{Basic vs. Derived rules}
\label{sec:basic}

A \define{derived rule} is a rule of proof that does not make any new proofs possible. Anything that can be proven with a derived rule can be proven without it. You can think of a short proof using a derived rule as shorthand for a longer proof that uses only the basic rules. 

A useful fact about our natural deduction system is that the basic rules themselves are enough to derive every SL validity. Derived rules are helpful for making some proofs shorter or more intuitive, but one could prove anything provable without them.

Headline news: you WON'T be able to enter any derived rules on Carnap in proofs or use on assignments. So this section is less important, but still provides some good examples of proof syntax and reasoning in a natural deduction system. 

%\section{Basic and derived rules}


%We have so far introduced five rules: Conditional Elimination, \emph{modus tollens}, Disjunction Elimination, Conjunction Elimination, and Conjunction Introduction. There are still more rules still to learn, but it is helpful first to pause and draw a distinction between different kinds of rules.

%Many of our rules, we have seen, carry the name `Elimination' or `Introduction', along with the name of one of our SL connectives. In fact, every rule we've seen so far except \emph{modus tollens} has had such a name. Such rules are the \emph{basic} rules in our natural deduction system. The basic rules comprise an Introduction and an Elimination rule for each connective, plus one more rule. \emph{Modus tollens} is NOT a basic rule; we will call it a \emph{derived} rule.

%A derived rule is a non-basic rule whose validity we can derive using basic rules only. 

%We have already seen the Introduction and Elimination rules for conjunction, and the elimination rules for disjunction and conditionals. In the next several sections, we'll finish canvassing the basic rules, then say a bit more about \emph{modus tollens} and other derived rules.

\subsection{A derived rule: \emph{modus tollens}}

Modus tollens is an extremely important and common inference rule in ordinary reasoning (based on the logical equivalence between a conditional $P \eif Q$ and its contrapositive $\enot Q \eif \enot P$. Here is what the derived rule Modus Tollens looks like:

\begin{proof}
	\have[m]{ab}{\metaA{}\eif\metaB{}}
	\have[n]{a}{\enot\metaB{}}
	\have[\ ]{b}{\enot\metaA{}} \by{MT}{ab,a}
\end{proof}

If you have a conditional on one numbered line, and the negation of its consequent on another line, you may derive the negation of its antecedent on a new line. We abbreviate the justification for this rule as `MT' for \emph{modus tollens}. If you know that if she found the treasure, she is happy, and you also know that she isn't happy, then you can very sensibly infer that she didn't find the treasure.

You may notice an asymmetry between the labels we use for \emph{modus ponens} and \emph{modus tollens}: in the former case, we use Conditional Elimination (\eif E) as its official name, but in the latter case, we call the rule Modus Tollens (MT). We'll explain in more detail why we use the labels in this way when we discuss `basic' and `derived' rules in \S\ref{sec:basic} below. For now, the important thing is to understand how to use the rule.

We do not include a horizontal line in the statement of the rule, because we don't want to assume that lines $m$ and $n$ are assumptions. They might be --- they might be premises in the argument, for example --- but they might themselves be derived, and have rules justifying them. It doesn't matter how we got $m$ and $n$, it just matters that we have them.

Here is an example employing \emph{Modus Tollens} several times over. We will derive $\enot A$ from $\{A \eif B, B \eif C, C \eif D, \enot D\}$:

\begin{proof}
	\hypo{ab}{A \eif B}
	\hypo{bc}{B \eif C}
	\hypo{cd}{C \eif D}
	\hypo{nd}{\enot D} \want{\enot A}
	\have{c}{\enot C} \by{MT}{cd,nd}
	\have{b}{\enot B} \by{MT}{bc,c}
	\have{a}{\enot A} \by{MT}{ab,b}
\end{proof}

At each of lines 5--7, we cite a conditional and the negation of its consequent to infer the negation of its antecedent.

\subsection{Deriving Modus tollens from our Basic Rules}

%We have been discussing \emph{modus tollens} throughout this chapter, and 

%In \S\ref{sec:reiteration} we showed how, in the case of one simple proof using \emph{modus tollens}, we can instead give a slightly longer proof using only basic rules. In fact, we can prove this much more generally, via this proof schema:

We can derive the modus tollens (MT) rule from our basic rules using the following proof schema:

\begin{proof}
	\hypo{p}{\metaA{}}
	\hypo{qnp}{\metaB{} \eif \enot \metaA{}} \want{\enot \metaB{}}
	\open
		\hypo{q}{\metaB{}} \by{Assume for reductio}{}
		\have{np}{\enot \metaA{}}\ce{qnp,q}
		\have{nnp}{\metaA{}}\by{R}{p}
	\close
	\have{nq}{\enot \metaB{}}\ni{q-nnp}
\end{proof}

As always,  \metaA{} and \metaB{} are meta-variables. They are not symbols of SL, but stand-ins for arbitrary sentences of SL. So this is not, strictly speaking, a proof in SL. It is more like a recipe; no matter what sentences you want to use \emph{modus tollens} on, if that rule would allow it, this pattern of proof will get you to the same place using basic rules only. This means that \emph{modus tollens} is not really necessary; we could prove everything we want to prove without it.

Nevertheless, it is a convenient shortcut. We add it to our list of derived rules.

\subsection{Derived Rule: Dilemma}


Here is the Dilemma rule:

\begin{proof}
	\have[m]{ab}{\metaA{}\eor\metaB{}} 
	\have[n]{ac}{\metaA{}\eif\metaC{}}
	\have[o]{bc}{\metaB{}\eif\metaC{}}
	\have[\ ]{c}{\metaC{}} \by{DIL}{ab,ac,bc}
\end{proof}

It might not be immediately obvious that this is a valid inference rule, but if you think about it a minute, you may be able to see why it is a good rule. If you know that two conditionals are true, and they have the same consequent, and you also know that one of the two antecedents is true, then whichever of those two disjuncts is true, will license a \emph{modus ponens} inference to the conclusion.

For example, suppose you know all of the following:

\begin{earg}
\item[] If it is raining, the car is wet.
\item[] If it is snowing, the car is wet.
\item[] It is raining or it is snowing.
\end{earg}

From these premises, you can definitely establish that the car is wet. This is the form that the Dilemma rule captures. We use the label `dilemma' to convey the idea that, whichever way you pick, you'll be stuck with the same conclusion. (``You're damned if you do, and you're damned if you don't...'')

The Dilemma rule is also derivable from the basic rules. The proof is a bit more complicated, but here it is in schematic form:

\begin{proof}
	\hypo{ab}{\metaA{}\eor\metaB{}}
	\hypo{ac}{\metaA{}\eif\metaC{}}
	\hypo{bc}{\metaB{}\eif\metaC{}}\by{want \metaC{}}{}
	\open
		\have{nc}{\enot \metaC{}}\by{for reductio}{}
		\open
			\hypo{a1}\metaA{}\by{for reductio}{}
			\have{c1}{\metaC{}}\ce{ac, a1}
			\have{nc1}{\enot\metaC{}}\by{R}{nc}
		\close
		\have{na}{\enot\metaA{}}\ni{a1-c1, a1-nc1}
		\open
			\hypo{b2}\metaB{}\by{for reductio}{}
			\have{c2}{\metaC{}}\ce{bc, b2}
			\have{nc2}{\enot\metaC{}}\by{R}{nc}
		\close
		\have{b}\metaB{}\by{DS}{ab, na}
		\have{nb}{\enot\metaB{}}\ni{b2-c2, b2-nc2}
	\close
	\have{c}{\metaC{}} \ne{nc-b, nc-nb}
\end{proof}

To understand this proof, think about the broad outline of it. Ultimately, we derive \metaC{} via Negation Elimination; that's why we assumed \enot\metaC{} on line 4, and worked our way to a contradiction on lines 12--13. The internal lines are subproofs to the intermediate conclusions \metaB{} and \enot\metaB{}. 

This is our first example of a natural deduction proof that involves nested assumptions. You can assume something a second new sentence even while a previous assumption is open, as this proof does at line 5. Make sure to close the assumptions in the proper order, closing the newest assumptions first. (Here, the line 5 assumption must be closed, as it is after line 7, before the line 4 one is, at the end of the proof.)

As in the case of \emph{modus tollens}, Dilemma doesn't allow us to prove anything we couldn't prove via basic rules. Anytime you wanted to use the Dilemma rule, you could always take ten extra lines and prove the same thing in basic rules. But it's a useful shorthand to include on our list of derived rules.

\subsection{Derived Rule: Hypothetical Syllogism}



We also add hypothetical syllogism (HS) as a derived rule:

\begin{proof}
	\have[m]{ab}{\metaA{}\eif\metaB{}}
	\have[n]{bc}{\metaB{}\eif\metaC{}}
	\have[\ ]{ac}{\metaA{}\eif\metaC{}}\by{HS}{ab,bc}
\end{proof}

We have already given a proof of one instance of Hypothetical Syllogism on p.~\pageref{HSproof}. The general proof schema is just the same, but with $P$, $Q$, and $R$ replaced with \metaA{}, \metaB{}, and \metaC{}, respectively.


\subsection{Derived Rule: Disjunctive Syllogism}

Recall this argument form from the Introduction of this chapter:

\begin{earg}
\item[] $P \eor Q$
\item[] $\enot P$
\item[\therefore] $Q$
\end{earg}

We noted above that this is a different argument form from one that uses \emph{modus ponens}; instead, it uses a kind of derivation that is specific to disjunction. From a disjunction like $P \eor Q$ alone, you can't conclude either disjunct, but you can conclude that \emph{at least one} of the two disjuncts is true. So if you have also established that one of the disjuncts is false --- i.e., its negation is true --- then you can conclude the other disjunct, as in the argument above.

This inference form is sometimes called `Disjunctive Syllogism' (DS).\footnote{In the UBC version of this book, Ichikawa christens this the official Disjunction Elimination ({\eor}E) rule. But for philosophical and pedagogical reasons, I have relegated it to a derived rule, in favor of proof-by-cases as our official {\eor}E rule.} If you have a disjunction and also the negation of one of its disjuncts, you may conclude the other disjunct.

\begin{multicols}{2}
\begin{proof}
	\have[m]{ab}{\metaA{}\eor\metaB{}}
	\have[n]{nb}{\enot\metaB{}}
	\have[\ ]{a}\metaA{} \by{Disjunctive Syllogism}{ab,nb}
\end{proof}

\begin{proof}
	\have[m]{ab}{\metaA{}\eor\metaB{}}
	\have[n]{na}{\enot\metaA{}}
	\have[\ ]{b}\metaB{} \by{DS}{ab,nb}
\end{proof}

\end{multicols}

We represent two different inference patterns here, because the rule allows you to conclude \emph{either} disjunct from the negation of the other. If we'd only listed the left version of the rule above, then DS would've only permited one to conclude the \emph{first} disjunct from the negation of the \emph{second} one, along with the disjunction. Our rule lets us work with either disjunction. (If you want to be very fussy about it, you could think of these as two different rules with a strong conceptual similarity that happen to have the same name.)

%Josh: would need to rewrite a diff example earlier on, not using disjunctive syllogism. 
%Now that we have several rules, we are in a position to see how they can interact to construct more interesting proofs. 

Consider for example this argument form:

\begin{earg}
\item[] $\enot L \eif (J \eor L)$
\item[] $\enot L$
\item[\therefore] $J$
\end{earg}

The two premises match the requirements for Conditional Elimination (\emph{modus ponens}). And once that is done, we will be in a position to use Disjunctive Syllogism  to derive the desired conclusion:

\begin{proof}
	\hypo{c}{\enot L \eif (J \eor L)}
	\hypo{a}{\enot L}  \want {J}
	\have{3}{J \eor L} \ce{c,a}
	\have{4}{J} \by{DS}{a,3}
\end{proof}

Via Conditional Elimination, we can derive $J \eor L$ from lines 1 and 2. Then from $J \eor L$ and $\enot L$, we can derive $J$, by Disjunctive Syllogism.

In this example, we used the premise on line 2 twice --- notice that the 2 appears in the justification for both line 3 and line 4. There is no limit to how many times you can make use of a given line in a natural deduction proof; once something is established, you can make use of it as often as you like.



\section*{Summary of definitions}
\begin{itemize}
\item A sentence \metaA{} is a \define{theorem} if and only if $\vdash\metaA{}$.

\item Two sentences \metaA{} and \metaB{} are \define{provably equivalent} if and only if $\metaA{}\vdash\metaB{}$ and $\metaB{}\vdash\metaA{}$.

\item $\{\metaA{}_1,\metaA{}_2,\ldots\}$ is \define{provably inconsistent} if and only if, for some sentence \metaB{}, $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash(\metaB{} \eand \enot \metaB{})$.
\end{itemize}


\subsection{Rules of replacement}

Consider how you would prove this argument form valid: $F\eif(G\eand H)$ \therefore\ $F\eif G$

Perhaps it is tempting to write down the premise and apply the {\eand}E rule to the conjunction $(G \eand H)$. This is impermissible, however, because the basic rules of proof can only be applied to whole sentences. In order to use {\eand}E, we need to get the conjunction $(G \eand H)$ on a line by itself. Here is a proof:

\begin{proof}
	\hypo{fgh}{F\eif(G\eand H)} \pr{}
	\open
		\hypo{f}{F}\as{want $G$}
		\have{gh}{G \eand H}\ce{fgh,f}
		\have{g}{G}\ae{gh}
	\close
	\have{fg}{F \eif G}\ci{f-g}
\end{proof}

The rules we have seen so far must apply to wffs that are on a proof line by themselves. We will now introduce some derived rules that may be applied to wffs that are parts of more complex sentences. These are called \define{rules of replacement}, because they can be used to replace part of a sentence with a logically equivalent expression. One simple rule of replacement is Commutativity (abbreviated Comm), which says that we can swap the order of conjuncts in a conjunction or the order of disjuncts in a disjunction. We define the rule thus:

\begin{center}
\begin{tabular}{rl}
$(\metaA{}\eand\metaB{}) \Longleftrightarrow (\metaB{}\eand\metaA{})$\\
$(\metaA{}\eor\metaB{}) \Longleftrightarrow (\metaB{}\eor\metaA{})$\\
$(\metaA{}\eiff\metaB{}) \Longleftrightarrow (\metaB{}\eiff\metaA{})$
& Comm
\end{tabular}
\end{center}

The double arrow means that you can take a subformula on one side of the arrow and replace it with the subformula on the other side. The arrow is double-headed because rules of replacement work in both directions. And replacement rules --- unlike all the rules we've seen so far --- can be applied to wffs that are part of more complex sentences. They don't need to be on their own line.

Consider this argument: $(M \eor P) \eif (P \eand M)$ \therefore\ $(P \eor M) \eif (M \eand P)$

It is possible to give a proof of this using only the basic rules, but it will be somewhat tedious. With the Comm rule, we can provide a proof easily:

\begin{proof}
	\hypo{1}{(M \eor P) \eif (P \eand M)} \pr{}
	\have{2}{(P \eor M) \eif (P \eand M)}\by{Comm}{1}
	\have{n}{(P \eor M) \eif (M \eand P)}\by{Comm}{2}
\end{proof}

(We need to apply the rule twice, because each application allows one transformation. We transformed the antecedent first, then the consequent. The opposite order would also have been fine.)

Another rule of replacement is Double Negation (DN). With the DN rule, you can remove or insert a pair of negations for any wff in a line, even if it isn't the whole line. This is the rule:

\begin{center}
\begin{tabular}{rl}
$\enot\enot\metaA{} \Longleftrightarrow \metaA{}$ & DN
\end{tabular}
\end{center}

Two more replacement rules  are called De Morgan's Laws, named for the 19th-century British logician August De Morgan. (Although De Morgan did formalize and publish these laws, many others discussed them before him.) The rules capture useful relations between negation, conjunction, and disjunction. Here are the rules, which we abbreviate DeM:

\begin{center}
\begin{tabular}{rl}
$\enot(\metaA{}\eor\metaB{}) \Longleftrightarrow (\enot\metaA{}\eand\enot\metaB{})$\\
$\enot(\metaA{}\eand\metaB{}) \Longleftrightarrow (\enot\metaA{}\eor\enot\metaB{})$
& DeM
\end{tabular}
\end{center}

As we have seen, $\metaA{}\eif\metaB{}$ is equivalent to $\enot\metaA{}\eor\metaB{}$. A further replacement rule captures this equivalence. We abbreviate the rule MC, for `material conditional.' It takes two forms:

\begin{center}
\begin{tabular}{rl}
$(\metaA{}\eif\metaB{}) \Longleftrightarrow (\enot\metaA{}\eor\metaB{})$ &\\
$(\metaA{}\eor\metaB{}) \Longleftrightarrow (\enot\metaA{}\eif\metaB{})$ & MC
\end{tabular}
\end{center}

Now consider this argument: $\enot(P \eif Q)$ \therefore\ $P \eand \enot Q$

As always, we could prove this argument valid using only the basic rules. With rules of replacement, though, the proof is much simpler:

\begin{proof}
	\hypo{1}{\enot(P \eif Q)} \pr{}
	\have{2}{\enot(\enot P \eor Q)}\by{MC}{1}
	\have{3}{\enot\enot P \eand \enot Q}\by{DeM}{2}
	\have{4}{P \eand \enot Q}\by{DN}{3}
\end{proof}

A final replacement rule captures the relation between conditionals and biconditionals. We will call this rule biconditional exchange and abbreviate it {\eiff}{ex}.

\begin{center}
\begin{tabular}{rl}
$[(\metaA{}\eif\metaB{})\eand(\metaB{}\eif\metaA{})] \Longleftrightarrow (\metaA{}\eiff\metaB{})$
& {\eiff}{ex}
\end{tabular}
\end{center}



%
%Although they don't do it in the book, I've been in the habit of writing $(\metaA{}\eand\metaB{}\eand\metaC{})$ and dropping the inner pair of parentheses. This is fine. If we'd wanted to, we could have defined the basic rules in a more general way:
%
%\begin{proof}
%	\have[n]{a1}{\metaA{}_1}
%	\have{2}{\metaA{}_2}
%	\have[\vdots]{1}{\vdots}
%	\have[n]{an}{\metaA{}_n}
%	\have[\ ]{aaa}{\metaA{}_1~\eand\ldots\eand~\metaA{}_n} \ai{}
%\end{proof}
%
%\bigskip
%\begin{proof}
%	\have{3}{\metaA{}_1~\eand\ldots\eand~\metaA{}_n}
%	\have{1}{\metaA{}_i} \ae{}
%\end{proof}
%
%\bigskip
%\begin{proof}
%	\have{1}\metaA{}
%	\have{3}{\metaA{}\eor\metaB{}_1\eor\metaB{}_2\ldots\eor\metaB{}_n} \ai{}
%\end{proof}
%
%We don't need these extended versions, since for any given n we could prove them as a derived rule.
%
%
%The basic rules for conjunction can be valuable in a proof even if there are no conjunctions in any of the assumptions; the basic rules for disjunction can be used even if there are no disjunctions in any assumptions; and similarly for the other basic rules. The rules for identity are different, in that there must be an identity claim in some assumption in order for the rules to do any work. Other than the trivial identity that we can introduce with the {=}I rule
%
%
%do not apply we can now prove that identity is \emph{transitive}: If $a=b$ and $b=c$, then $a=c$. The proof proceeds in this way:
%\begin{proof}
%	\open
%		\hypo{p}{a=b \eand b=c}\by{want $a=c$}{}
%		\have{ab}{a=b}\ae{p}
%		\have{bc}{b=c}\ae{p}
%		\have{ac}{a=c}\by{{=}E}{ab,bc}
%	\close
%	\have{conc}{(a=b \eand b=c)\eif a=c} \ci{p-ac}
%\end{proof}
%
%
%As an example, consider this argument:
%\begin{quote}
%There is only one button in my pocket. There is a blue button in my pocket. Therefore, there is no button in my pocket that is not blue.
%\end{quote}
%We begin by defining a symbolization key:
%\begin{ekey}
%\item{UD:} buttons in my pocket
%\item{Bx:} $x$ is blue.
%\end{ekey}
%\begin{proof}
%	\hypo{one}{\forall x\forall y\ x=y}
%	\hypo{eb}{\exists x Bx} \by{want $\enot\exists x \enot Bx$}{}
%	\open
%		\hypo{be1}{Be}
%		\have{ef1}{e=f}\Ae{one}
%		\have{bf1}{Bf}\by{{=}E}{ef1,be1}
%	\close
%	\have{bf}{Bf}\Ee{eb,be1-bf1}
%	\have{ab}{\forall x Bx}\Ai{bf}
%	\have{nnab}{\enot\enot\forall x Bx}\by{DN}{ab}
%	\have{nenb}{\enot\exists x\enot Bx}\by{QN}{nnab}
%\end{proof}

\iffalse
 
\clearpage



\practiceproblems

\solutions
\problempart
\label{pr.justifySLproof}
Provide a justification (rule and line numbers) for each line of proof that requires one {\color{black}(which for \textit{Carnap} means ALL of them!)}
\begin{multicols}{2}
\begin{proof}
\hypo{1}{W \eif \enot B}
\hypo{2}{A \eand W}
\hypo{2b}{B \eor (J \eand K)}
\have{3}{W}{}
\have{4}{\enot B} {}
\have{5}{J \eand K} {}
\have{6}{K}{}
\end{proof}

\begin{proof}
\hypo{1}{L \eiff \enot O}
\hypo{2}{L \eor \enot O}
\open
	\hypo{a1}{\enot L}
	\have{a2}{\enot O}{}
	\have{a3}{L}{}
	\have{a4}{\enot L}{}
\close
\have{3}{L}{}
\end{proof}

\begin{proof}
\hypo{1}{Z \eif (C \eand \enot N)}
\hypo{2}{\enot Z \eif (N \eand \enot C)}
\open
	\hypo{a1}{\enot(N \eor  C)}
	\have{a2}{\enot N \eand \enot C} {}
	\open
		\hypo{b1}{Z}
		\have{b2}{C \eand \enot N}{}
		\have{b3}{C}{}
		\have{b4}{\enot C}{}
	\close
	\have{a3}{\enot Z}{}
	\have{a4}{N \eand \enot C}{}
	\have{a5}{N}{}
	\have{a6}{\enot N}{}
\close
\have{3}{N \eor C}{}
\end{proof}
\end{multicols}

\solutions
\problempart
\label{pr.solvedSLproofs}
Give a proof for each argument in SL.
\begin{earg}
\item $K\eand L$, \therefore $K\eiff L$
\item $A\eif (B\eif C)$, \therefore $(A\eand B)\eif C$
\item $P \eand (Q\eor R)$, $P\eif \enot R$, \therefore $Q\eor E$
\item $(C\eand D)\eor E$, \therefore $E\eor D$
\item $\enot F\eif G$, $F\eif H$, \therefore $G\eor H$
\item $(X\eand Y)\eor(X\eand Z)$, $\enot(X\eand D)$, $D\eor M$ \therefore $M$
\end{earg}

\problempart
Give a proof for each argument in SL.
\begin{earg}
\item $Q\eif(Q\eand\enot Q)$, \therefore\ $\enot Q$
\item $J\eif\enot J$, \therefore\ $\enot J$
\item $E\eor F$, $F\eor G$, $\enot F$, \therefore\ $E \eand G$
\item $A\eiff B$, $B\eiff C$, \therefore\ $A\eiff C$
\item $M\eor(N\eif M)$, \therefore\ $\enot M \eif \enot N$
\item $S\eiff T$, \therefore\ $S\eiff (T\eor S)$
\item $(M \eor N) \eand (O \eor P)$, $N \eif P$, $\enot P$, \therefore\ $M\eand O$
\item $(Z\eand K) \eor (K\eand M)$, $K \eif D$, \therefore\ $D$
\end{earg}


\solutions
\problempart
\label{pr.SLND.theorems}
Show that each of the following sentences is a theorem in SL.
\begin{earg}
\item $O \eif O$
\item $N \eor \enot N$
\item $\enot(P\eand \enot P)$
\item $\enot(A \eif \enot C) \eif (A \eif C)$
\item $J \eiff [J\eor (L\eand\enot L)]$
\end{earg}

\problempart
Show that each of the following pairs of sentences are provably equivalent in SL.
\begin{earg}
\item $\enot\enot\enot\enot G$, $G$
\item $T\eif S$, $\enot S \eif \enot T$
\item $R \eiff E$, $E \eiff R$
\item $\enot G \eiff H$, $\enot(G \eiff H)$
\item $U \eif I$, $\enot(U \eand \enot I)$
\end{earg}

\solutions
\problempart
\label{pr.solvedSLproofs2}
Provide proofs to show each of the following.
\begin{earg}
\item $M \eand (\enot N \eif \enot M) \vdash (N \eand M) \eor \enot M$
\item \{$C\eif(E\eand G)$, $\enot C \eif G$\} $\vdash$ $G$
\item \{$(Z\eand K)\eiff(Y\eand M)$, $D\eand(D\eif M)$\} $\vdash$ $Y\eif Z$
\item \{$(W \eor X) \eor (Y \eor Z)$, $X\eif Y$, $\enot Z$\} $\vdash$ $W\eor Y$
\end{earg}



\problempart
For the following, provide proofs using only the basic rules. The proofs will be longer than proofs of the same claims would be using the derived rules.
\begin{earg}
\item Show that MT is a legitimate derived rule. Using only the basic rules, prove the following: \metaA{}\eif\metaB{}, \enot\metaB{}, \therefore\ \enot\metaA{}
\item Show that Comm is a legitimate rule for the biconditional. Using only the basic rules, prove that $\metaA{}\eiff\metaB{}$ and $\metaB{}\eiff\metaA{}$ are equivalent.
\item Using only the basic rules, prove the following instance of DeMorgan's Laws: $(\enot A \eand \enot B)$, \therefore\ $\enot(A \eor B)$
\item Show that {\eiff}{ex} is a legitimate derived rule. Using only the basic rules, prove that $D\eiff E$ and $(D\eif E)\eand(E\eif D)$ are equivalent.
\end{earg}




\problempart
\begin{earg}
\item If you know that $\metaA{}\vdash\metaB{}$, what can you say about $(\metaA{}\eand\metaC{})\vdash\metaB{}$? Explain your answer.
\item If you know that $\metaA{}\vdash\metaB{}$, what can you say about $(\metaA{}\eor\metaC{})\vdash\metaB{}$? Explain your answer.
\end{earg}

\fi


%****************STUFF JH CUT FROM UBC EDITION*******************************


\iffalse 

\section{Ichikawa: The remaining basic rules}

All of the rules introduced in this chapter are summarized in the Quick Reference guide at the end of this book.

\subsection{Disjunction Introduction}
If $M$ is true, then $M \eor N$ must also be true. In general, the Disjunction Introduction rule ({\eor}I) allows us to derive a disjunction if we already have one of its two disjuncts:

\begin{proof}
	\have[m]{a}\metaA{}
	\have[\ ]{ab}{\metaA{}\eor\metaB{}}\oi{a}
	\have[\ ]{ba}{\metaB{}\eor\metaA{}}\oi{a}
\end{proof}

One can introduce a disjunct in either position --- it can be the first disjunct or the second disjunct. Accordingly, both options are listed here. (One is not required to do both; you can just take whichever version you find most helpful.)

As always, \metaB{} can be \emph{any} sentence whatsoever. So the following is a legitimate proof:

\begin{proof}
	\hypo{m}{M}
	\have{mmm}{M \eor ([(A\eiff B) \eif (C \eand D)] \eiff [E \eand F])}\oi{m}
\end{proof}

It may seem odd that just by knowing $M$ we can derive a conclusion that includes sentences like $A$, $B$, and the rest --- sentences that have nothing to do with $M$. Yet the conclusion follows immediately by {\eor}I. This is as it should be: The truth conditions for the disjunction mean that, if \metaA{} is true, then $\metaA{}\eor \metaB{}$ is true regardless of what \metaB{} is. So the conclusion could not be false if the premise were true; the argument form is valid.



\subsection{Conditional Introduction}

Consider this argument:
\begin{earg}
\item[] $R \eor F$
\item[\therefore] $\enot R \eif F$
\end{earg}
The argument form seems like it should be valid --- either $R$ or $F$ is true, so if $R$ isn't true, $F$ must be. (You can confirm that it is valid by examining the truth tables.) The Conditional Introduction rule can demonstrate that this is so.

We begin the proof by writing down the premise of the argument, making a note of our intended conclusion, and drawing a horizontal line, like this:

\begin{proof}
	\hypo{rf}{R \eor F} \want{\enot R \eif F}
\end{proof}

If we had $\enot R$ as a further premise, we could derive $F$ by the {\eor}E rule. But we do not have $\enot R$ as a premise of this argument, nor can we derive it directly from the premise we do have --- so we cannot simply prove $F$. What we will do instead is start a \emph{subproof}, a proof within the main proof. When we start a subproof, we draw another vertical line to indicate that we are no longer in the main proof. Then we write in an assumption for the subproof. This can be anything we want. Here, it will be helpful to assume $\enot R$. We want to show that, if we did assume that, we would be able to derive $F$. So we make a new assumption that $\enot R$, and give ourselves a note that we wish to derive $F$. Our proof now looks like this:

\begin{proof}
	\hypo{rf}{R \eor F}  \want{\enot R \eif F}
	\open
		\hypo{nr}{\enot R} \want {F}
		\have{}{}
	\close
\end{proof}

It is important to emphasize that we are not claiming to have \emph{proven} $\enot R$ from the premise on line 1. We do not need to write in any justification for the assumption line of a subproof. (The `want' is a note to ourself, not a justification.) The new horizontal line indicates that a new \emph{assumption} is being made; we also include a vertical line that extend to future lines, to indicate that this assumption remains in effect. You can think of the subproof as posing the question: What could we show \emph{if} $\enot R$ were true? We are trying to show that we could derive $F$. And indeed, we can:

\begin{proof}
	\hypo{rf}{R \eor F}  \want{\enot R \eif F}
	\open
		\hypo{nr}{\enot R} \want {F}
		\have{f}{F}\by{DS}{rf, nr}
	\close
\end{proof}

This has shown that \emph{if} we had $\enot R$ as a premise, \emph{then} we could prove $F$. In effect, we have proven that $F$ follows from $\enot R$. This is indeed very close to demonstrating the conditional, $\enot R \eif F$. This last step is just what the Conditional Introduction rule will allow us to perform. We close the subproof and derive $\enot R \eif F$ in the main proof. Our final proof looks like this:

\begin{proof}
	\hypo{rf}{R \eor F}
	\open
		\hypo{nr}{\enot R}\want {F}
		\have{f}{F}\by{DS}{rf, nr}
	\close
	\have{nrf}{\enot R \eif F}\ci{nr-f}
\end{proof}

The {\eif}I rule lets us \define{discharge} the assumption we'd been making, ending that vertical line. We also stop indenting --- the difference in placement of lines 3 and 4 emphasizes that they are importantly different: during lines 2 and 3, we were \emph{assuming} that \enot $R$. By the time we get to line 4, we are no longer making that assumption.

Notice that the justification for applying the {\eif}I rule is the entire subproof. That's why we justify it by reference to a range of lines, instead of a comma-separated list. Usually that will be more than just two lines.

It may seem as if the ability to assume anything at all in a subproof would lead to chaos: Does it allow you to prove any conclusion from any premises? The answer is no, it does not. Consider this proof schema:

\begin{proof}
	\hypo{a}\metaA{}
	\open
		\hypo{b1}\metaB{}
		\have{b2}{\metaB{}\eand \metaA{}} \ai{a,b1}
	\close
\end{proof}

Does this show that one can conjoin any arbitrary sentence \metaB{} with premise \metaA{}? After all, we've written \metaB{}\eand\metaA{} on a line of a proof that began with \metaA{}, without violating any of the rules of our system. The reason this doesn't have that implication is the vertical line that still extends into line 3. That line indicates that the assumption made at line 2 is still in effect. When the vertical line for the subproof ends, the subproof is \emph{closed}. In order to complete a proof, you must close all of the subproofs. The conclusion to be proved must not be `blocked off' by a vertical line; it should be aligned with the premises.

In this example, there is no way to close the subproof and show that the conjunction follows from line 1 alone. One can only close a subproof via particular rules that allow you to do so; {\eif}I is one such rule; {\eand}I does not close subproofs. One can't just close a subproof willy-nilly. Closing a subproof is called \emph{discharging} the assumptions of that subproof. So we can put the point this way: You cannot complete a proof until you have discharged all of the assumptions other than the original premises of the argument.

Of course, it is legitimate to do this:

\begin{proof}
	\hypo{a}\metaA{}
	\open
		\hypo{b1}\metaB{}
		\have{b2}{\metaB{}\eand \metaA{}} \ai{a,b1}
	\close
	\have{c}{\metaB{} \eif (\metaB{}\eand\metaA{})} \ci{b1-b2}
\end{proof}

This should not seem so strange, though. The conclusion on line 4 really does follow from line 1. (Draw a truth table if you need convincing of this.) 

Once an assumption has been discharged, any lines that have been shown to follow from that assumption --- i.e., those lines inside the box indicated by the vertical line of that assumption --- cannot be cited as justification on further lines. So this development of the proof above, for instance, is not permitted:

\begin{proof}
	\hypo{rf}{R \eor F}
	\open
		\hypo{nr}{\enot R}\want {F}
		\have{f}{F}\by{DS}{rf, nr}
	\close
	\have{nrf}{\enot R \eif F}\ci{nr-f}
	\have{bad}{F \eor A}\oi{f}
\end{proof}

Once the assumption made at line 2 has been discharged at line 4, the lines within that assumption --- 2 and 3 --- are unavailable for further justification. So one cannot perform Disjunctive Syllogism  on line 3 at line 5. Line 3 was not demonstrated to follow from the premise on line 1 --- it follows only from this combined with the \emph{assumption} on line 2. And by the time we get to line 5, we are no longer making that assumption.

Put in its general form, the {\eif}I rule looks like this:

\begin{proof}
	\open
		\hypo[m]{a}\metaA{} \by{want \metaB{}}{}
		\have[n]{b}\metaB{}
	\close
	\have[\ ]{ab}{\metaA{}\eif\metaB{}}\ci{a-b}
\end{proof}

When we introduce a subproof, we typically write what we want to derive off to the right. This is just so that we do not forget why we started the subproof if it goes on for five or ten lines. There is no `want' rule. It is a note to ourselves, and not formally part of the proof.

Although it is consistent with the natural deduction rules to open a subproof with any assumption you please, there is some strategy involved in picking a useful assumption. Starting a subproof with a random assumption is a terrible strategy. It will just waste lines of the proof. In order to derive a conditional by the {\eif}I rule, for instance, you must assume the antecedent of that conditional.

The {\eif}I rule also requires that the consequent of the conditional be the last line of the subproof. It is always permissible to close a subproof and discharge its assumptions, but it will not be helpful to do so until you get what you want. This is an illustration of the observation made above, that unlike the tree method, the natural deduction method requires some strategy and thinking ahead.

You should \emph{never} make an assumption without a plan for how to discharge it.





\subsection{Biconditional Introduction}
Biconditionals indicate that the two sides have the same truth value. One establishes a biconditional by establishing each direction of it as conditionals. To derive $W \eiff X$, for instance, you must establish both $W \eif X$ and $X \eif W$. (You might derive those conditionals via {\eif}I, or you might get them some other way. They might even simply be premises.) Those conditionals may occur in either order; they need not be on consecutive lines. (Compare the shape of the {\eand}I rule.) Schematically, the Biconditional Introduction rule works like this:

\begin{proof}
	\have[m]{ab}{\metaA{}\eif\metaB{}}
	\have[n]{ba}{\metaB{}\eif\metaA{}}
	\have[\ ]{c}{\metaA{}\eiff\metaB{}} \bi{ab, ba}
\end{proof}


\subsection{Biconditional Elimination}

The Biconditional Elimination rule ({\eiff}E) is a generalized version of \emph{modus ponens} ({\eif}E). If you have the left-hand subsentence of the biconditional, you can derive the right-hand subsentence. If you have the right-hand subsentence, you can derive the left-hand subsentence. This is the rule:



\begin{multicols}{2}
\begin{proof}
	\have[m]{ab}{\metaA{}\eiff\metaB{}}
	\have[n]{a}\metaA{}
	\have[\ ]{b}\metaB{} \be{ab,a}
\end{proof}
\begin{proof}
	\have[m]{ab}{\metaA{}\eiff\metaB{}}
	\have[n]{a}\metaB{}
	\have[\ ]{b}\metaA{} \be{ab,a}
\end{proof}
\end{multicols}

As in the case of Disjunction Elimination, we include both versions under the same name, so that you don't need to worry about whether the side you already have is the left-hand side of the biconditional or the right-hand side. Whichever side it is, you may derive the other via Biconditional Elimination.

\fi 

