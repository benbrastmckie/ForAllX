%!TEX root = ../../forallx-mit.tex
\chapter{Natural Deduction in PL}
\label{ch.PL-deduction}

% TODO: prove cut
% TODO: use the language of 'admissible rules' and 'basic rules'

This chapter introduces a natural deduction proof system for $\PL$ which we will refer to as \textit{Propositional Logic} (PL).
This is the logic of sentences which aims to describe formal reasoning which is valid in virtue of the logical form of the sentences involved where the syntactic primitives are all sentences or sentential operators.
In a future chapter, we will introduce \textit{First-Order Logic} (FOL) for $\FOL$ which includes predicates, constants, variables, and quantifiers. 
Until then, we will continue to restrict attention to what can be expressed with the resources included in $\PL$.
Accordingly, PL does not aim to describe \textit{all} of formal reasoning, but rather only the formal reasoning that can be carried out in $\PL$.

In Chapter \ref{ch.PL-semantics}, the logical consequence relation $\vDash$ provided a first answer to the question of what logic aims to study.
Additionally, we presented the truth table method and semantic proof method for establishing which logical consequences hold and which do not.
Nevertheless, these methods leave something to be desired.
To begin with, we saw just how poorly the truth table method scales with the number of sentence letters, making this method practically infeasible for more than four sentence letters.
Although the semantic proof method did not face this same problem, semantic proofs were often cumbersome to write where their construction was completely unconstrained.
That is, we never said, and indeed cannot say, what counts as an adequate semantic proof.
Rather, these proofs took place in our metalanguage mathematical English which does not have clear cut boundaries or rules.

% % TREE LOGIC
% Whereas the previous chapter provided two methods 
% The tree method has advantages and disadvantages.
% One advantage of trees is that, for the most part, they can be produced in a purely mechanical way where no flash of insight is necessary.
% Another advantage is that producing a complete open tree provides a recipe for constructing an interpretation that satisfies the root.
% A disadvantage of the tree method is that trees do not provide an intuitive line of reasoning from the premises to the conclusion. 
% Although a closed tree shows that the premises together with the negation of the conclusion leads to a contradiction, we don't learn what it is about the premises which lead to the conclusion.

Recall the strategies for writing semantic proofs that we began to describe in the previous chapter.
For instance, these included proof by contradiction and proof by cases.
This raises a question about what are \textit{all} of the strategies that one might employ along these lines.
More than strategies, we want to know what are \textit{all} of the moves that we can make when writing a proof, and what inferences are absolutely basic and cannot be subdivided into further steps.
Questions of these kinds lead to a completely different approach to our present inquiry into the nature of formal reasoning.
Instead of asking what is a logical consequence of what by quantifying over the interpretations of a language, we may seek to describe a collection of basic inferences, chaining these together in order to say what can be inferred from what.
% By contrast to the semantic (or model theoretic) approach that we have already seen, 
This chapter will be concerned to answer this question by providing a proof system for $\PL$.

After considering a number of arguments in English in Chapter \ref{ch.introduction}, we observed that natural languages do not have well-defined boundaries, frustrating any attempt to say something completely general about all sentences and arguments in English.
Chapter \ref{ch.PL-syntax} avoided this problem by presenting the artificial language $\PL$ which has a well-defined notion of a wfs that we may use to regiment English sentences and arguments.
Although regimentation itself remains a matter of judgment with no definite answers, this method nevertheless provided a way to identify the logical forms that explain why certain patterns of reasoning in English are especially compelling.
In particular, we defined the interpretations of $\PL$ to be functions from the wfss of $\PL$ to truth-values, drawing on this definition in order to introduce logical consequence along with a number of other logical properties and relations.
However, none of this would have been possible were we to attempt these definitions for English.
 
In just the same way that it was important to work with an artificial language in order to provide a mathematically precise definition of logical consequence, it will also be important to draw on a well-defined language in order to describe the basic inferences that hold in virtue of their logical forms.
Rather than introducing another artificial language, we will continue to work with $\PL$ maintaining all of the definitions from before in order to provide the proof system PL, also called a \textit{logic}, for reasoning in $\PL$. 
Functionally, you can think of PL as including rules somewhat akin to the proof strategies and steps that we used in writing informal proofs before, only now they will take a precise mathematical form.

What of our informal proofs from before?
Are they to be trusted given that they are written in the vague natural language English together with some mathematical conventions? 
What of mathematics proofs in general which are also written in this kind of language?
Are these really mere approximations whose validity is only to be accounted for by regimenting them in some more precise language?
You might be surprised to know that the answer is `No'.

Rather than encoding some final truth, the logical systems that we will present are better understood to be abstractions from the intuitive bedrock from which we must begin: natural language, and in our case, English.
After all, how would you ever hope to learn what the sentential operators (much less the sentence letters) of $\PL$ mean? 
The semantic answer we provided above used mathematical English to do so, and this was no mistake since meanings have to get going somewhere and in this respect $\PL$ is no place to begin.

Instead of undercutting the meanings that you understand in English, introducing formal languages provides a way to distill certain elements of meaning that we have reason to care about even if they depart from their correlates in natural language. 
In analogy, you can think of this like refining the raw materials found in nature into the sorts of materials that are of considerable use to us in constructing the build environment.
Rather than the material world, our concern is with the conceptual world, and what we are doing here is a kind of conceptual engineering.
Although these are only metaphors, hopefully they will help to shed some light on what we have been doing and will continue to do throughout this course.
In particular, it is important to appreciate that English cannot be given up any more than the natural world around us.
Accordingly, we will continue to write informal proofs to establish claims about our object language $\PL$.
Soon we will have an analogue for also proving things \textit{in} $\PL$.

Whereas the semantic clauses for $\PL$ drew on our grasp of certain elements of mathematical English in order to provide a systematic way to \textit{interpret} the wfss of $\PL$, this chapter will also draw on mathematical English in order describe how to \textit{reason} in $\PL$.
One way to think about our target here is to contemplate the extension of the logical consequence relation $\vDash$.
That is, think of the set of ordered pairs which relate any set of wfss of $\PL$ to a further wfs of $\PL$ where the latter is a logical consequence of the former, or in set notation: $\set{\tuple{\MetaG, \metaA} : \MetaG \vDash \metaA}$.
Needless to say, this is a large, though not unruly space.
Although our definition of logical consequence $\vDash$ provides an essential account of this space of logical consequences, it is hard work to check which wfss are logical consequences of which sets of wfss of $\PL$. 
Thus it would be convenient to streamline the process by which we may determine whether $\MetaG \vDash \metaA$.

As with all of our methods, writing formal proofs in $\PL$ has its range of natural applications where sometimes it is more trouble than it is worth and a semantic proof would have been better.
Nevertheless, convenience is not our only motivation.
Rather, what we should like to describe are the most basic inferences that make up the practice of formal reasoning, composing those inferences in order to not only say what follows from what in virtue of logical form, but also how. % from a radically different perspective.
Of course, not any rational seeming maneuvers ought to be included.
In particular, we will require the basic inferences that make up PL to be valid.
This is referred to as \textit{soundness}: if $\metaA$ can be inferred from $\MetaG$, then $\metaA$ is a logical consequence of $\MetaG$. 
However, this is not all of what we want.
Rather, we also want our basic inferences to be inherently compelling.
Put otherwise, we are looking to find the atoms that make up formal reasoning.

Insofar as the proof systems that we will be concerned with in this course aim to encode the natural patterns of formal reasoning, we will refer to these systems as \define{natural deduction} systems.
In particular, PL is a natural deduction system for $\PL$ where later chapters will consider a natural deduction system for $\FOL$.
These systems provide a way to argue from the premises to a conclusion in logically valid ways while resembling natural forms of reasoning.
In addition to being familiar, reasoning in this way helps to illustrate the logical connections between various claims in a way that is compelling \textit{on its own terms}.
That is, you don't have to take a course in logic or learn the semantics for an artificial language in order to appreciate the patterns of reasoning that we will consider, finding them compelling.

% TREE STUFF
% By contrast with the tree method, PL is intended to model human reasoning, 
% Consequently, working through a natural deduction proof requires a bit more insight than a tree proof does.
% Although natural deduction proofs can be used to prove that an argument is valid, natural deduction system will not tell you if an argument is invalid, nor will it produce an interpretation that invalidates the argument.
% That is, there is no equivalent to a completed open tree in natural deduction.
%
% Despite these disadvantages, there is good reason to care about natural deduction systems. % independent of what metalogical properties they may be shown to have.
% Recall that the only reason we provided for caring about $\PL$ tree proofs is that the tree proof system was shown to be sound and complete.
% Accordingly, tree proofs can be used to determine something that we already care about, i.e., which $\PL$ arguments are logically valid.
% By contrast, we may argue that the natural deduction rules included in PL are forced upon us by the meanings of the connectives included in $\PL$.
% For instance, given that $A\eand B$, one may deduce $A$ since what it is for a conjunction like $A\eand B$ to be true is for both of its conjuncts to be true, and so in particular we may conclude that $A$ is true. 
% Given a compelling range of inference rules for each of the connectives in $\PL$, one may ask whether it is possible to argue from the premises to the conclusion.
% This is to ask whether an argument is \textit{deductively valid}, where knowing the answer to such a question holds interest independent of what metalogical properties the natural deduction system in question may be shown to have.
%
% It will turn out that there is a natural deduction derivation in PL corresponding to every valid $\PL$ argument.
% Put otherwise, PL is, like the tree method, complete.
% Moreover, there are no derivations in PL of a conclusion that fails to be entailed by its premises, and so PL is also sound.
% Nevertheless, when an argument is invalid, there is no way to use PL to see that it is valid.
% Instead, we get stuck trying different ways to derive the conclusion from the premises, though in the case of an invalid argument, no such attempt will succeed.

In what follows, we will introduce ten basic derivation rules for the five sentential operators in $\PL$.
Each operator will have an introduction and an elimination rule which, taken together, describe a certain dimension of the \textit{meaning} of that operator.
In particular, the introduction and elimination rules describe how to reason with the operators in $\PL$.
There is also a rule for introducing assumptions and a trivial rule for reiterating earlier lines of a proof.
% This corresponds to the idea that we can always conclude $A$ if $A$ has already been derived.
% However trivial reiteration, such an inference rule will turn out to have an important role to play in our proof system.
Given these twelve rules in all, we will be in a position to provide a precise definition of a proof in PL, where it is for this reason that PL is referred to as a proof system, or \textit{logic} for $\PL$.

As finite and familiar as all of this will turn out to be, PL may nevertheless be shown to have some remarkable properties.
In addition to showing that $\metaA$ is a logical consequence of $\MetaG$ whenever $\metaA$ can be proved from $\MetaG$, we will also show that nothing is missing: our proof system is capable of deriving \textit{all} logical consequences whatsoever. 
That is, PL is \textit{complete} in addition to being \textit{sound}.
We will prove these results in later chapters.

% The following section will present the basic rules of inference included in PL.
% In contrast to trees, PL proofs are sequences of sentences satisfying certain properties.
% Already there is something much more natural about sequences of sentences instead of trees.
% After all, natural reasoning takes place in time, and time is linear.



% \section{Natural Deduction}
%
% The basic idea of a natural deduction proof is simple.
% You begin by writing down the premises that you are arguing from, numbering each in turn.
% We will then make use of the eleven rules alluded to above.
% If you can use the rules to derive a sentence from what you have already written down, then you can add this derived sentence on a new numbered line.
% If, following the rules, you manage to derive the conclusion from the premises, then you've shown that the argument from the premises to the conclusion is \textit{deductively valid}.
%
% We will provide a precise definition of a proof in PL once we have introduced the basic rules of inference.
% However, to get a sense of things, consider the following $\PL$ arguments:
%
% \begin{multicols}{2}
% \emph{Modus Ponens:}
% \begin{earg}
% \item $P \eif Q$
% \item $P$
% \item[\therefore] $Q$
% \end{earg}
%
% \emph{Disjunctive Syllogism:}
% \begin{earg}
% \item $P \eor Q$
% \item $\enot P$
% \item[\therefore] $Q$
% \end{earg}
%
% \end{multicols}
%
% Both are valid; you could confirm this with a four-line truth table.
% Either would demonstrate that there is no interpretation satisfying both premises, while falsifying the conclusion.
% The truth table method does not distinguish between these argument forms; it simply shows that they are both valid.
% There is, however, an interesting and important difference between these two argument forms, gestured at by their labels.
% The first argument, \emph{Modus Ponens}, has a distinctive syntactic form: its premises are a conditional and the antecedent of that conditional, and the conclusion is the consequent; the second has a different form.
%
% The natural deduction method is based on the recognition of particular kinds of valid forms.
% They also correspond reasonably well to familiar forms of informal reasoning.
% If you know a conditional, and you also know its antecedent, it is easy to infer its consequent.
% Imagine being sure that if I ate the chilli, I'll get sick, and also being sure that I ate the chilli.
% You will surely infer that I will get sick.
% \emph{Modus ponens} is the name of this kind of conditional reasoning, and there is a special rule for it in our natural deduction system.

\section{Premises and Assumptions}
\label{sec:PremiseAssumption}

Before introducing the rules, it will help to get a sense of what PL proofs look like in order to articulate some important constraints on the lines of a proof to which a rule may appeal.

A PL proof begins with a (possibly empty) list of premises, where these will be indicated by writing `:PR' on the right.
It is often helpful to include a note of what you intend to derive at this point.
For instance, consider the following list of premises:
\begin{fitchproof}
  \hypo{1}{A \eif (B \eif C)} \pr{}
  \hypo{2}{A} \pr{}
\end{fitchproof}
% A line of a proof is \define{live} just in case it is not \define{dead}.
% It is important to take note of the vertical line which abuts the premises included above.
% Were we to apply one of our rules (in this case conditional elimination discussed below), we can only appeal to sentences which are \define{live} abut this vertical line, or abut a vertical line to the left.
% We will refer to such sentences as \define{live} and falling within the \define{scope} of application of our rules.
% In this case, there is only one vertical line where the sentences in the first two lines abut the vertical line, and so both sentences fall within the scope of the rules.
The horizontal line indicates where the premises end and the rest of the derivation begins.
For instance, we may apply conditional elimination (discussed below) to derive the following:
\begin{fitchproof}
  \hypo{1}{A \eif (B \eif C)} \pr{}
  \hypo{2}{A} \pr{}
  \have{3}{B \eif C} \ce{1,2}
\end{fitchproof}
Note that we appealed to lines $1$ and $2$ in order to derive line $3$, indicating as much in the justification of line $3$. 
If $B \eif C$ is all that we wanted to derive, then we would be done. 
However, suppose that we were to continue by adding a new assumption.
\begin{fitchproof}
  \hypo{1}{A \eif (B \eif C)} \pr{}
  \hypo{2}{A} \pr{}
  \have{3}{B \eif C} \ce{1,2}
  \open
    \hypo{4}{B} \as{}
  \close
\end{fitchproof}
At any point in a proof, we can introduce a new assumption on an indented line and starting a new vertical line.
More precisely, consider the \textit{assumption rule} (AS) below:
\factoidbox{
\begin{fitchproof}
  \open
	\hypo[\ ]{c}{\metaA} \as{}
  \close
\end{fitchproof}}
We will refer to the process of adding an assumption as one of \define{opening a subproof}.
Subproofs are what they sound like: a proof within a proof, starting from a single assumption added anywhere in a proof rather than the premises with which we began the proof.
For instance, we might add the following lines to the proof above:
\begin{fitchproof}
  \hypo{1}{A \eif (B \eif C)} \pr{}
  \hypo{2}{A} \pr{}
  \have{3}{B \eif C} \ce{1,2}
  \open
    \hypo{4}{B} \as{}
    \have{5}{C} \ce{3,4}
    \have{6}{C \eand A} \ai{2,5}
  \close
  \have{7}{B \eif (C \eand A)} \ci{4-6}
\end{fitchproof}
Line $5$ applies conditional elimination (discussed below) on lines $3$ and $4$, and then line $6$ applies conjunction introduction (also discussed below) to lines $2$ and $5$.
We then close the subproof, where this may take place at any point in the subproof by ending the vertical line and stepping back one level of indentation.
Once a subproof closes, the lines of that closed subproof are \define{dead}, and so cannot be appealed to individually.
Nevertheless, we may appeal to the subproof in its entirety, where line $7$ does just this, using conditional introduction (discussed below) which cites lines $4$--$6$ (note the hyphen in place of the comma).

Every line of a proof that is not dead is referred to as \define{live}, where rules that cite individual lines (as opposed to subproofs) can only appeal to lines that are live at that point in the proof.
For instance, were we to continue our proof a little further, we could not appeal to lines $4$, $5$, or $6$ since these lines are dead.
Thus we stipulate the following restriction:
  \factoidbox{
    \textsc{Citation:} For a rule to cite a single line, that line must not occur within a subproof that has been closed before the line where the rule is being applied.
  }
Closing a subproof is also called \define{discharging} the assumption of that subproof.
Subproofs allow us to think about what we could show if we made a further assumption.
Accordingly, we have to be careful to keep track of what assumptions we are making and when it is and is not permitted to appeal to an assumption or the wfs of $\PL$ that we can derive from that assumption.
Our Fitch-style proof system accomplishes this task graphically by indenting assumptions and drawing vertical lines along the length of the resulting subproof.
The details for each rule which makes use of this feature of our proof system will be discussed below, but it is important to have some sense of all of this before introducing the rules.





\section{Reiteration}
\label{sec:reiteration}

The first rule was already eluded to above.
Given any wfs of $\PL$ on a live line of a proof, the \emph{reiteration rule} (R) allows you to repeat that wfs on a new line.
\begin{fitchproof}
	\have[$\vdots$]{}{\vdots}
	\have[4]{a1}{A \eand B}
	\have[$\vdots$]{}{\vdots}
	\have[10]{a2}{A \eand B} \by{R}{a1}
\end{fitchproof}
Given that we have written `$A \eand B$' on line $4$, we may repeat this wfs at some later line, e.g., line $10$.
We also add a citation which justifies what we have written.
In this case, we write `R', to indicate that we are using the reiteration rule, and we write `$4$', to indicate that we have applied it to line $4$.
Here is the general expression of the reiteration rule R:
\factoidbox{
\begin{fitchproof}
	\have[m]{a}{\metaA}
	\have[\ ]{c}{\metaA} \by{R}{a}
\end{fitchproof}}
If $\metaA$ occurs on any line $m$ within the scope of application, we can reiterate $\metaA$, justifying this addition by writing `:$m$ R' to indicate that reiteration was applied to line $m$.
Of course, in an actual proof, the lines are numbered, and so $m$ will take on a numerical value.

Here is an example of three legal applications of rule R followed by an illegal application:

\begin{fitchproof}
		\hypo{r}{P} \pr{}
		\open
			\hypo{l}{\enot Q} \as{}
			\have{rl}{\enot Q} \by{R (\textsc{legal})}{l}
			\have{n} {P} \by{R (\textsc{legal})}{r}
			\close
      \have{m} {\enot Q \eif P} \ci{l-n}
      \have{k} {P} \by{R (\textsc{legal})}{r}
    \have{con}{\enot Q}\by{R, (\textsc{illegal})}{l}
	\end{fitchproof}

% Once you have derived something from the premise, that new line is available to help justify future lines.
% It is important to observe that every line after the premises includes a justification starting with a colon `:' which is followed by the relevant line numbers and the derivation rule in question, in that order.
On the second line, we begin a subproof by assuming $\enot Q$.
You can reiterate $\enot Q$ within the subproof as in line $3$, but not when you leave the subproof as in line $7$.
On line $4$, we reiterate $P$ on line $1$, maintaining the indentation of the subproof.
We then close the subproof, citing the subproof in line $5$.
At line $6$, we can reiterate line $1$ which is live, but at line $7$ we cannot appeal to line $2$ since this line is now dead.
Even if we were to open another subproof, we still could not appeal to line $2$.
Rather, the lines of a closed subproof are forever dead. 
Even so, this does not stop us from appealing to the subproof as a whole as we do in line $5$.



\section{Conjunction}

Consider the rule for \textit{conjunction introduction} ($\eand$I):

\factoidbox{
\begin{proof}
	\have[m]{a}\metaA{}
	\have[n]{b}\metaB{}
	\have[\ ]{c}{\metaA{}\eand\metaB{}} \ai{a, b}
\end{proof}}

This rule says that given any wfss $\metaA$ and $\metaB$ of $\PL$ on live lines, you may derive their conjunction $\metaA \eand \metaB$.
It is worth noting that $m$ and $n$ need not be consecutive lines, nor do they need to appear in the order listed.
We require only that each line has been established somewhere in the proof, and that both lines are live at the line in which we are applying the rule.

Whereas conjunction introduction licenses the derivation of a conjunction from any two wfss of $\PL$, conjunction elimination lets us do the opposite.
Given any live conjunction, we may derive either of its conjuncts.
For instance, if $A \eand (P \eor Q)$ is live, we may derive $A$, or we may derive $P \eor Q$, but we must choose which.
If we wish to derive both, then two applications of the rule is required, though the order does not matter.

Here are the left and right \textit{conjunction elimination} ($\eand$E) rules:
\factoidbox{
\begin{proof}
	\have[m]{ab}{\metaA{}\eand\metaB{}}
	\have[\ ]{a}\metaA{} \ae{ab}
	\have[\ ]{b}\metaB{} \ae{ab}
\end{proof}}
These rules allow us to derive either conjunct.
Although we will often end up deriving both conjunts, we need not do so.
For instance, this is a perfectly acceptable proof:

\begin{proof}
	\hypo[1]{ab}{A \eand B} \pr{}
	\have[2]{b}B \ae{ab}
\end{proof}

Note that the $\eand$E rule only requires one wfs of $\PL$, and so there is only one line number in the justification.
In order to see the conjunction rules to work together, consider the argument:
\begin{earg}
  \uitem{$[(A\eor B)\eif(C\eor D)] \eand [(E \eor F) \eif (G\eor H)]$}
  \eitem{$[(E \eor F) \eif (G\eor H)] \eand [(A\eor B)\eif(C\eor D)]$}
\end{earg}
The main logical operator in both the premise and conclusion is conjunction.
Since conjunction is commutative, the argument is obviously valid since the two conjunctions have the same two conjuncts in the opposite order.
In order to provide a proof, we begin by writing down the premise on a numbered line indicating that it is a premise.
Since this is the only premise, we draw a horizontal line where everything below this line must be justified by a proof rule.
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}} \pr{}
\end{proof}
From the premise, we can separate the conjuncts with $\eand$E.
This yields the following:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}} \pr{}
	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
\end{proof}
The $\eand$I rule requires that each of the conjuncts is live somewhere in the proof from the current line, though their order and distance from each other does not matter.
By applying the $\eand$I rule to lines $3$ and $2$, we may arrive at the desired conclusion.

\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}} \pr{}

	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
	\have{ba}{{[}(E \eor F) \eif (G\eor H){]} \eand {[}(A\eor B)\eif(C\eor D){]}} \ai{b,a}
\end{proof}

This proof may not look terribly interesting or surprising, but it shows how we can use the proof rules together to demonstrate the validity of an argument.
Note that using a truth table to show that this argument is valid would have required a staggering 256 lines, since there are eight sentence letters in the argument.
A semantic proof would be less unwieldy, but would not have been as simple or natural of an argument.
At the very least, you would have to know a bit about the semantics for our language $\PL$.
By contrast, the steps in the proof above are already pretty compelling given that `$\eand$' expresses conjunction.





\section{Disjunction}

Suppose Ludwig is reactionary.\footnote{This section has been adapted from the Calgary remix \S16.7.}
Then Ludwig is either reactionary or libertarian.
Trivial as this may seem, it speaks to the logic of disjunction.
Just as we may derive either conjunct from a conjunction, we may derive a disjunction from either of its disjuncts.

Thus the \textit{disjunction introduction} ($\eor$I) rule may be stated as follows:
\factoidbox{\begin{fitchproof}
	\have[m]{a}{\metaA}
	\have[\ ]{ab}{\metaA\eor\metaB}\oi{a}
	\have[\ ]{ba}{\metaB\eor\metaA}\oi{a}
\end{fitchproof}}
As above, the line $m$ must be live, where we cite this line in the justification of the rule application. 
Since $\metaB$ can be \emph{any} wfs of $\PL$, the following is a perfectly acceptable proof:
\begin{fitchproof}
	\hypo{m}{M} \pr{}
	\have{mmm}{M \eor ([(A\eiff B) \eif (C \eand D)] \eiff [E \eand F])}\oi{m}
\end{fitchproof}
Using a truth table to show this would have taken 128 lines.

The disjunction elimination rule is slightly trickier.
Suppose that either Ludwig is reactionary or he is libertarian.
It does not follow that Ludwig is reactionary, for he might be a libertarian.
Equally, we cannot conclude that Ludwig is libertarian, since he might be reactionary.
Given that we don't know which disjunct is true, it is difficult to deduce anything from a disjunction on its own.
The elimination rule for disjunction provides a workaround.

Suppose that we could show that if Ludwig's is reactionary, then he is an Austrian economist.
Suppose that we could also show that if Ludwig's is a libertarian, then he is also an Austrian economist.
Even though we don't know whether Ludwig is reactionary or a libertarian, it doesn't matter: in either case he is an Austrian economist.
This is a natural way to make use of a disjunction even when we don't know which disjunct is true.
Indeed, we employed reasoning of this kind in the semantic proof by cases that we gave in Chapter \ref{ch.PL-semantics}.
Generalizing on this line of reasoning, consider the following \textit{disjunction elimination} ($\eor$E) rule:
\factoidbox{
	\begin{fitchproof}
		\have[m]{ab}{\metaA\eor\metaB}
		\open
			\hypo[i]{a}{\metaA} {} \as{for \eor E}
			\have[\vdots]{d1}{\vdots}
			\have[j]{c1}{\metaC}
		\close
		\open
			\hypo[k]{b}{\metaB}{} \as{for \eor E}
			\have[\vdots]{d2}{\vdots}
			\have[l]{c2}{\metaC}
		\close
		\have[ ]{c}{\metaC}\oe{ab, a-c1,b-c2}
	\end{fitchproof}}
This rule is somewhat clunkier to write down than our previous rules, but the idea is a natural one.
Suppose that we have some disjunction $\metaA \eor \metaB$.
Suppose that we can also construct two subproofs showing that $\metaC$ can be derived from the assumption that $\metaA$, and that $\metaC$ can be derived from the assumption that $\metaB$.
We can then infer $\metaC$ from the original disjunction $\metaA \eor \metaB$ together with our two subproofs.
As usual, there can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$.
Moreover, the subproofs and the disjunction can come in any order, and do not have to be adjacent to each other as above.
Although the lines $i$--$j$ and $k$--$l$ belong to closed subproofs and so dead, line $m$ must be live.

Some examples will help illustrate.
Consider the following argument:
\begin{earg}
  \uitem{$(P \eand Q) \eor (P \eand R)$}
  \eitem{$P$}
\end{earg}
A proof might run like this, adding the notes `for $\eor$E' to improve readability:
	\begin{fitchproof}
		\hypo{prem}{(P \eand Q) \eor (P \eand R) } \pr{}
			\open
				\hypo{pq}{P \eand Q} \as{for \eor E}
				\have{p1}{P}\ae{pq}
			\close
			\open
				\hypo{pr}{P \eand R} \as{for \eor E}
				\have{p2}{P}\ae{pr}
			\close
		\have{con}{P}\oe{prem, pq-p1, pr-p2}
	\end{fitchproof}
Here is a slightly harder example.
Consider the following argument:
\begin{earg}
  \uitem{$A \eand (B \eor C)$}
  \eitem{$(A \eand B) \eor (A \eand C)$}
\end{earg}
We may then construct the following proof:
	\begin{fitchproof}
		\hypo{aboc}{A \eand (B \eor C)} \pr{}
		\have{a}{A}\ae{aboc}
		\have{boc}{B \eor C}\ae{aboc}
		\open
			\hypo{b}{B} \as{for \eor E}
			\have{ab}{A \eand B}\ai{a,b}
			\have{abo}{(A \eand B) \eor (A \eand C)}\oi{ab}
		\close
		\open
			\hypo{c}{C} \as{for \eor E}
			\have{ac}{A \eand C}\ai{a,c}
			\have{aco}{(A \eand B) \eor (A \eand C)}\oi{ac}
		\close
	\have{con}{(A \eand B) \eor (A \eand C)}\oe{boc, b-abo, c-aco}
	\end{fitchproof}
As natural as the rules may seem in isolation, it is not always obvious how to put them together to get from some premises to a conclusion.
Like any skill, the ability to construct PL proofs requires practice.
To help, we will cover some strategies for finding proofs at the end of the chapter.
Nevertheless, once a natural deduction proof has been constructed, each step is easy to justify, making the derivation in total impervious to doubts.
Moreover, this certainty does not stem from any semantic considerations.
Rather, the proof rules are directly justified by our intuitive understanding of how to use the sentential operators in our language.




\section{Conditional Introduction}

The rule for conditional introduction has already been used in the examples used to first set up the proof system, and should have felt both compelling an familiar.
% Here is an abbreviated version of the reasoning which showed up in the completeness proof:
% 	\begin{quote}
% 		Assume $\MetaG \nproves \bot$.
%     Given the lemmas we have proven, it follows from this assumption that $\MetaG \nmodels \bot$.
%     Although we don't know whether $\MetaG \nmodels \bot$ holds independent of our assumption, we may conclude that \textit{if} $\MetaG \nproves \bot$, \textit{then} $\MetaG \nmodels \bot$. 
% 	\end{quote}
The idea here is that you help yourself to something that you may not know is true, do some reasoning to arrive at some further claim, then conclude by asserting a conditional claim: if the assumption is true, then the further claim is true.
This conclusion follows despite not knowing if the original assumption is true.
Here is a concrete example of this type of reasoning:
	\begin{quote}
		Ludwig is reactionary. Therefore if Ludwig is libertarian, then Ludwig is both reactionary and libertarian.
	\end{quote}
We may regiment this argument as a natural deduction proof by starting with one premise $R$ for `Ludwig is reactionary':
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
	\end{fitchproof}
We may now make an additional assumption $L$ for `Ludwig is libertarian'.
In common parlance, we might use the turn of phrase `Suppose for the sake of argument that\ldots', or when writing informal proofs, we might start off with `Assume $R$ for conditional proof'.
In PL, we will indicate that we are adding an assumption by writing `AS' on the right, where it is often helpful to also include `for $\eif$Intro' as a note to yourself or your reader.
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
		\open
			\hypo{l}{L} \as{for \eif Intro}{}
	\end{fitchproof}

Note that we are not claiming to have proved $L$ from line 1.
Accordingly, we do not write any justification for the additional assumption on line 2.
Rather, we have started a new subproof by indenting the wfs $L$ and starting a new vertical line.
We have also underlined $L$ since it is playing a role analogous to a premise in our new subproof. 

With this extra assumption in place, we are now in a position to use $\eand$I from before.
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
		\open
			\hypo{l}{L} \as{for \eif Intro}{}
			\have{rl}{R \eand L}\ai{r, l}
%			\close
%		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{fitchproof}
Given the assumption $L$, we have deduced $R \eand L$.
We may now discharge our assumption, closing the subproof and adding an appropriate conditional on the next line.
	\begin{fitchproof}
		\hypo{r}{R} \pr{}
		\open
			\hypo{l}{L} \as{for \eif Intro}{}
			\have{rl}{R \eand L}\ai{r, l}
			\close
		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{fitchproof}
Whereas the indented subproof carries out reasoning under the assumption of $L$, line $4$ reverts back to our original proof which carries out reasoning under the assumption of our single premise $R$.
Accordingly, we cannot conclude $R\eand L$ merely under the assumption of $R$ by writing $R\eand L$ at the original level of indenting.
Nevertheless, we can assert the conditional $L\eif(R\eand L)$ as given in $4$, justifying this line by referring to the entire subproof rather than to individual lines of our proof.
In this case, there are only two lines in the subproof, but in general there may be many more.
Even in the case where the subproof only consists of two lines, we must use a hyphen to indicate that we are citing a subproof instead of two lines.

Generalising on this pattern, consider the \textit{conditional introduction} rule ($\eif$I):
\factoidbox{
	\begin{fitchproof}
		\open
			\hypo[i]{a}{\metaA} \as{}
			\have[\vdots]{b}{\vdots}
			\have[k]{c}{\metaB}
		\close
		\have[\ ]{ab}{\metaA\eif\metaB}\ci{a-c}
	\end{fitchproof}}
% Whereas the individual lines that a proof rule may appeal to must be live, subproofs consist of dead lines.
By appealing to the subproof as a whole for justification, we may write a conditional in a new line stepping back one level of indentation where the assumption of the subproof occurs as the antecedent of the conditional and the conclusion of the subproof occurs as the consequent.
% In particular, we may justify a conditional claim with a subproof which begins with the antecedent and ends with the consequent, citing the subproof it is entirety.
As we will see, knowing what the rule is one thing and knowing when to use it is another.


\section{Conditional Elimination}

Many different arguments demonstrate the classic inference \emph{modus ponens}:

\begin{multicols}{3}
\begin{earg}
  \eitem{$P \eif \enot Q$}
  \uitem{$P$ \quad\quad }
  \eitem{$\enot Q$}
\end{earg}

\begin{earg}
  \eitem{$\enot P \eif (A \eiff B)$}
  \uitem{$\enot P$ \quad\quad }
  \eitem{$A \eiff B$}
\end{earg}

\begin{earg}
  \eitem{$(P \eor Q) \eif A$}
  \uitem{$P \eor Q$ \quad\quad }
  \eitem{$A$}
\end{earg}

\end{multicols}

The natural deduction system of this chapter will include a rule of inference corresponding to \emph{modus ponens} which goes by the name \textit{conditional elimination} ($\eif$E).
Here is the rule:
\factoidbox{
\begin{proof}
	\have[m]{ab}{\metaA{}\eif\metaB{}}
	\have[n]{a}\metaA{}
	\have[\ ]{b}\metaB{} \ce{ab,a}
\end{proof}}
What this rule says is that if you have a conditional $\metaA\eif\metaB$ on a live line number $m$, and you also have the antecedent $\metaA$ of that conditional on a live line $n$, you can write the consequent $\metaB$ on a new line.
In order to justify this inference, we will list the line numbers $m$ and $n$ as well as `$\eif$E' to specify the rule.
Given the conditional elimination rule, we can prove that the arguments given above are valid.
Here are proofs of two of them:


\begin{multicols}{2}

\begin{proof}
	\hypo{if}{P \eif \enot Q} \pr{}
	\hypo{a}P \pr{}
	\have{c}{\enot Q} \ce{if,a}
\end{proof}


\begin{proof}
	\hypo{if}{(P \eor Q) \eif A} \pr{}
	\hypo{a}{P \eor Q} \pr{}
	\have{c}A \ce{if,a}
\end{proof}

\end{multicols}

Notice that these proofs share the same structure.
We start by listing the premises followed by a horizontal line, where subsequent lines will need to be derived with the rules.
We then apply the conditional elimination rule to get the conclusion, citing the appropriate lines.
One can produce more complicated proofs with the same rule.

\begin{earg}
  \eitem{$A$ }
  \eitem{$A \eif B$ }
  \eitem{$B \eif C$ }
  \uitem{$C \eif [\enot P \eiff (Q \eor R)]$ }
  \eitem{$\enot P \eiff (Q \eor R)$}
\end{earg}

We begin by writing our four premises on numbered lines:

\begin{proof}
	\hypo{4}{A} \pr{}
	\hypo{1}{A \eif B} \pr{}
	\hypo{2}{B\eif C} \pr{}
	\hypo{3}{C \eif [\enot P \eiff (Q \eor R)]} \pr{(Want \enot $P \eiff (Q \eor R)$)}
%	\have{b} {B} \ce{1,4}
%	\have{c}{C} \ce{2,b}
%	\have{}{\enot P \eiff (Q \eor R))} \ce{3,c}
\end{proof}

The parenthetical off to the right is optional, but can help to keep track of the conclusion that we are attempting to establish.
The proof will be complete once we derive $\enot P \eiff (Q \eor R)$ by applying the rules to the premises or lines that result from doing so.
Since we cannot use conditional elimination to get to our desired conclusion directly from our premises, it is worth considering what we can do.
For instance, we can use conditional elimination on lines $1$ and $2$ to get $B$ on a new line, and then repeat using our new line together with line $3$ to get $C$ on yet another new line. 
Continuing in this manner gives us the following proof:

\begin{proof}
	\hypo{4}{A} \pr{}
	\hypo{1}{A \eif B} \pr{}
	\hypo{2}{B\eif C} \pr{}
	\hypo{3}{C \eif [\enot P \eiff (Q \eor R)]} \pr{want \enot P \eiff (Q \eor R)}
	\have{b} {B} \ce{4,1}
	\have{c}{C} \ce{2,b}
	\have{}{\enot P \eiff (Q \eor R))} \ce{3,c}
\end{proof}

Having derive line $5$ from lines $1$ and $2$, we may derive $6$ from $3$ and $5$, and then conclude by deriving $7$ from $4$ and $6$.
In general, each time that we appeal to earlier lines in a proof in order to apply a rule, we must check to see if those lines are live.
However, in this case, we have not introduced any assumptions, and so there is no risk that any lines fail to be live.

In order to see the conditional introduction and elimination rules work together, consider:
\begin{earg}
  \eitem{$P \eif Q$}
  \uitem{$Q \eif R$}
  \eitem{$P \eif R$}
\end{earg}
We start by listing the premises--- this much is automatic requiring no thinking whatsoever.
But now we have to think about where we are going, i.e., we want to conclude with the conditional $P\eif R$.
A great way to do this is by conditional introduction and so, to use this rule, we must begin by assuming the antecedent $P$ of the conditional we want to conclude.
\begin{fitchproof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P} \as{for \eif I}{}
	\close
\end{fitchproof}
Note that there is nothing preventing us from appealing to $P$ in the course of our subproof since before we have closed the subproof, all of its lines are still live.
Thus we have:
\label{HSproof}
\begin{fitchproof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P} \as{for \eif I}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{fitchproof}
Whereas line $4$ derives $Q$ from lines $1$ and $3$ by conditional elimination, we may apply the same rule to derive $R$ on line $5$ from the lines $2$ and $4$.
Finally, we may close our subproof, and conclude $P \eif R$ on line $6$ while citing the subproof on lines $3$--$5$. 
Knowing when exactly to open a subproof can take some practice, but a good rule of thumb is that if you want to establish a conditional either at the end of a proof or along the way, you may well need to assume its antecedent and reason your way to the consequent.






\section{The Biconditional}

The rules for the biconditional will be like double-barrelled versions of the rules for the conditional.
In order to prove $F \eiff G$  you must be able to prove $G$ on the assumption $F$, and separately, prove $F$ on the assumption $G$.
The \textit{biconditional introduction} rule ($\eiff$I) therefore requires two subproofs.
Schematically, the rule looks like this: 
\factoidbox{
\begin{fitchproof}
	\open
		\hypo[i]{a1}{\metaA} \as{for \eiff I}
		\have[\vdots]{c1}{\vdots}
		\have[j]{b1}{\metaB}
	\close
	\open
		\hypo[k]{b2}{\metaB} \as{for \eiff I}
		\have[\vdots]{c2}{\vdots}
		\have[l]{a2}{\metaA}
	\close
	\have[\ ]{ab}{\metaA\eiff\metaB}\bi{a1-b1,b2-a2}
\end{fitchproof}}
There can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$.
Moreover, the subproofs can come in any order, and the second subproof does not need to come immediately after the first.

The biconditional elimination rule ($\eiff$E) lets you do a bit more than the conditional rule.
If you have the left-hand subsentence of the biconditional, you can obtain the right-hand subsentence.
If you have the right-hand subsentence, you can obtain the left-hand subsentence.
\factoidbox{
\begin{fitchproof}
	\have[m]{ab}{\metaA\eiff\metaB}
	\have[n]{a}{\metaA}
	\have[\ ]{b}{\metaB} \be{ab,a}
\end{fitchproof}}
Equally, we may work in the reverse direction:
\factoidbox{\begin{fitchproof}
	\have[m]{ab}{\metaA\eiff\metaB}
	\have[n]{a}{\metaB}
	\have[\ ]{b}{\metaA} \be{ab,a}
\end{fitchproof}}
Note that in the citation for $\eiff$E, we always cite the biconditional first and either the left or right argument depending as the second argument.



\section{Negation}

Here is a simple mathematical argument:

\begin{earg}
  \eitem{Assume there is some greatest natural number, call it $n$.}
  \eitem{Now consider its successor $n+1$ which is also a natural number.}
  \eitem{Since $n+1 > n$, we may conclude that $n$ is not the greatest natural number. }
  \uitem{But this contradicts our assumption. }
  \eitem{Thus there is no greatest natural number.}
\end{earg}

We used \textit{reductio} style arguments of this kind in some of the semantic proofs in Chapter \ref{ch.PL-semantics}. 
The full Latin name \emph{reductio ad absurdum} means ``reduction to absurdity.''
Proofs of this form are also sometimes called \textit{indirect proofs}.
A \textit{reductio} argument assumes something which we would like to show is false and aims to derive a contradiction.
For instance, we might end up reaching the negation of the \textit{reductio} assumption, or else two wfss of the form $\metaB$ and $\enot\metaB$.
Given such a contradiction, we may assert the negation of the original assumption.

In mathematics, \textit{reductio} arguments often lead to conclusions like $0=1$ that contradict something that is already known more generally though the negation $0\neq 1$ might not show up anywhere in the proof.
Whether stated or not, what is going on here is that we really have two contradictory claims: $0=1$ and $0\neq 1$, or to be even more explicit, $\enot(0=1)$.
Mathematical proofs typically suppress many of the obvious details, and so do not take the form of fully explicit valid arguments of the kind with which we will be concerned.

The negation rules will allow us to write \textit{reductio} style arguments.
Like the conditional introduction rule (\eif I), the negation rules require a new assumption on an indented line, starting a new vertical line.
If this assumption can be shown to lead to both a wfs of $\PL$ as well as its negation within the course of the subproof, then we may write the negation of the assumption of this subproof on a new line, stepping back one level of indentation.
Schematically, this is what the \textit{negation introduction} ($\enot$I) rule looks like:

%try the  \ellipsesline command line to insert dots more easily!

\factoidbox{
\begin{proof}
\open
	\hypo[m]{na}\metaA{} \as{for \enot I}
	\have[n]{b}\metaB{}
	\have[o]{nb}{\enot\metaB{}}
\close
\have[\ ]{a}[\ ]{\enot\metaA{}}\ni{na-nb} %note that UBC has a more complex citation convention: {na-b, na-nb}
\end{proof}}

On line $m$, we assume $\metaA$ for \emph{reductio}.
Our goal is to derive a contradiction, represented by two wfss $\metaB$ and $\enot\metaB$ of $\PL$ on separate lines in any order.
Accordingly, it is often convenient to include a note to ourselves and our readers that we are trying to introduce a negation by reaching a contradiction.
Observe that $\metaB$ could be the same wfs as $\metaA$, e.g. both could be $P$, but this need not always be the case. 
Once we have derived a contradictory pair of wfss of $\PL$, we may close the subproof, moving to the left one level of indentation.
We may then write the negation of the assumption in the subproof $\enot\metaA$ on a new line, citing the whole subproof by using a hyphen and indicating the negation introduction rule $\enot$I.

Suppose that we want to derive an instance of the law of non-contradiction: $\enot(G \eand \enot G)$.
A decent rule of thumb is that if you want to conclude a negated wfs of $\PL$, it is natural to assume the negand and see if you can reach a contradiction, though this may not always be the best strategy.
However, in the case of $\enot (G \eand \enot G)$, this is just what we will do, starting a subproof by adding the assumption $G\eand \enot G$ to a proof without any premises.

\begin{proof}
	\open 
		\hypo{gng}{G\eand \enot G}\as{for $\enot$I}
		\have{g}{G}\ae{gng}
		\have{ng}{\enot G}\ae{gng}
	\close
	\have{ngng}{\enot(G \eand \enot G)}\ni{gng-ng}
\end{proof}

Although some proofs require some real creativity, this one is pretty obvious once we make the right assumption.
After all, the only rule we could apply to our assumption is $\eand$E, where two applications give us a contradiction.
By applying $\enot$I, we may conclude the proof.

The \textit{negation elimination} ($\enot$E) rule works in much the same way.
If we assume $\enot\metaA$ and show that it leads to a wfs of $\PL$ and its negation, we may conclude $\metaA$.
So the rule looks like this:

\factoidbox{
\begin{proof}
\open
	\hypo[m]{na}{\enot\metaA{}}\as{for \enot E}{}
	\have[n]{b}\metaB{}
	\have[o]{nb}{\enot\metaB{}}
\close
\have[\ ]{a}[\ ]\metaA{}\ne{na-nb}
\end{proof}}

As in the case of negation introduction, it is important that the justification of an application of negation elimination cite an entire subproof, indicating as much with a hyphen between the first and last lines.
Additionally, it is important that the contradictory pair of wfss of $\PL$ occur in the subproof itself rather than elsewhere in the proof.
Below is an example which makes an essential appeal to the reiteration rule in order to apply negation elimination:

\begin{proof}
	\hypo{p}{P} \pr{}
	\hypo{qnp}{\enot Q \eif \enot P} \pr{want $Q$}
	\open
		\hypo{q}{\enot Q} \as{for $\enot$E}{}
		\have{np}{\enot P}\ce{qnp,q}
		\have{nnp}{P}\by{R}{p}
	\close
	\have{nq}{Q}\ne{q-nnp}
\end{proof}

Negation elimination requires that one show that some wfs of $\PL$ and its negation are derivable given the assumption of a negated wfs of $\PL$.
In this case, we establish that $\enot P$ follows from the assumption that $\enot Q$ by conditional elimination.
Even though $P$ occurs on a live line, we must use the reiteration rule in order to include $P$ in our subproof.
Only then may we draw $Q$ as a conclusion by way of negation elimination. 

% \section{Exact Matches}
%
% Conditional elimination, as well as all of our other natural deduction rules, are syntactically defined.
% That is to say, the application of the rules depends on the exact shape of the $\PL$ sentences in question.
% Here, again, is the formal statement of the rule:
%
% \begin{proof}
% 	\have[m]{ab}{\metaA{}\eif\metaB{}}
% 	\have[n]{a}\metaA{}
% 	\have[\ ]{b}\metaB{} \ce{ab,a}
% \end{proof}
%
% It says that any time one has, on one line, a sentence made up of some sentence \metaA{}, followed by the `\eif' symbol, followed by some sentence \metaB{}, where one also has \metaA{} on another line, one may derive \metaB{}. This is the only pattern of inference that this rule permits. \metaA{} and \metaB{} can be any sentences of $\PL$, but a line justified by Conditional Elimination must fit this pattern exactly. It is not enough that one can `just see' that a given sentence follows via a similar pattern of inference.
%
% For example, this is \emph{not} a legal derivation in our system:
%
% \begin{proof}
% 	\hypo{ab}{P \eif (A \eand B)} \pr{}
% 	\hypo{a}{P} \pr{}
% 	\have{b}{B}\by{ILLEGAL \eif E}{ab,a}
% \end{proof}
%
% The Conditional Elimination rule requires that the new sentence derived be the consequent of the conditional cited. But in this example, $B$ is not the consequent of $P\eif (A \eand B)$ --- $A \eand B$ is. It is true that $B$ obviously follows from $A \eand B$, but the Conditional Elimination rule doesn't allow you to derive things just because they obviously follow. (Neither does any other rule in our formal system.) To derive $B$ from these premises we'll need to use another rule. (In particular, we will want to use the Conjunction Elimination rule, given below.)
%
% To check to make sure you are applying the rules correctly, one good heuristic is to think about whether you are relying on the rule itself, or on your intuitive understanding of the meanings of the symbols we use in $\PL$. Your intuitive understanding is a good way to think about which rules to use, but to check to make sure you're using the rules properly, think about whether the rules' exact formulations could explain why it is permissible to extend the derivation in the exact way you're working with. Pretend, for instance, that you have no idea what the `\eif' symbol means, but you do know that if you have two sentences joined by it on one line, and the first of those two sentences on another line, then you are allowed to copy down the second sentence exactly on a new line. This --- and no more --- is what the Conditional Elimination rule permits you to do. (This is what we mean when we say the rule is syntactically defined.) It would be pretty trivial to write a computer program to check to see whether a line is properly derived via Conditional Elimination (unlike us, computers are VERY good at following rules, since computers are defined via rules).

\section{Proof Strategy}
  \label{sec.PL.Strategy}

The examples have been relatively simple so far, but perhaps you can already get a sense of the kinds of strategic thinking that natural deduction proofs sometimes require.
For instance, although it is always permissible to open a subproof with any assumption, knowing which assumption to introduce and when to do so can require some care.
Starting a subproof with any arbitrary assumption may clutter your proof.
In order to obtain a conditional by $\eif$I, for example, it makes sense to assume the antecedent of the conditional in a subproof and see if you can derive the consequent.
This is an example of a good proof strategy.

It is also always permissible to close a subproof, discharging its assumptions.
However, it will not be helpful to do so until you have reached something useful.
Once the subproof is closed, you can only cite the entire subproof in a justification for a line following that subproof.
Those rules that call for a subproof, or multiple subproofs, require that the last line of the subproof is a wfs of $\PL$ of some form or other.
For instance, you are only allowed to cite a subproof for $\eif$I if the line you are justifying is of the form $\metaA \eif \metaB$ where $\metaA$ is the assumption of your subproof and $\metaB$ is the last line of your subproof.
This constrains the strategies that one might hope to employ in attempting to construct proofs in PL.

Getting good at natural deduction will take some practice.
The good news is that natural deduction proofs are a lot more interesting to construct than truth tables, and a much more beneficial skill: practicing natural deduction will streamline your reasoning well beyond the scope of this course, where the same cannot be said for filling out arrays with $1$s and $0$s.
Although there are no fail-safe methods, and certainly no substitute for practice, there are some general rules of thumb and strategies that are worth keeping in mind.


\paragraph{Work backwards from what you want.}
The ultimate goal is to derive the conclusion.
Look at the conclusion and ask what the introduction rule is for its main operator.
This gives you an idea of what should happen just before the last line of the proof.
Then you can treat this line as if it were your goal, asking what you could do to derive this new goal.

For example, if your conclusion is a conditional $\metaA \eif \metaB$, plan to use the $\eif$I rule.
This requires starting a subproof in which you assume $\metaA$.
In the subproof, you want to derive $\metaB$.

\paragraph{Work forwards from what you have.}
When you are starting a proof, look at the premises and consider what implications they might have, or what you would need to derive in order to make use of the premises.
It can help to think about the elimination rules for the main operators of the premises, or the wfss that you have derived so far.

For example, if you have a conditional $\metaA\eif\metaB$, and you also have $\metaA$ on a line, $\eif$E is a pretty natural move to make.
Sometimes it is a lot trickier to know what to do next, but not always.

% \paragraph{Change what you are looking at.}
% Replacement rules can often make your life easier. If a proof seems impossible, try out some different substitutions.
%
% For example: It is often difficult to prove a disjunction using the basic rules. If you want to show $\metaA{}\eor\metaB{}$, it is often easier to show $\enot\metaA{}\eif\metaB{}$ and use the MC rule.
%
% Some replacement rules should become second nature. If you see a negated disjunction, for instance, you should immediately think of DeMorgan's rule.

\paragraph{Repeat as necessary.}
A long proof is just a number of short proofs linked together, so you can fill the gap by alternately working back from the conclusion and forward from the premises.
Once you have decided how you might be able to get to the conclusion, ask what you might be able to do with the premises.
Then consider these new targets, asking how you might reach each them, slowly connecting the dots of your proof.

\paragraph{Try a reductio when nothing else works.}
If you cannot find a way to show something directly, try assuming its negation and see where this leads.
Sometimes this can help unlock a proof, perhaps even leading you to a direct line of argument.

\paragraph{Persist.}
Try different things.
If one approach fails, then try something else.
In general, there are typically many ways to construct a proof.



% \section{Subproofs}

% Having introduced all of the rules of PL,
Although these guidelines can help if you get stuck, it is worth mentioning some of the mistakes that are easy to make.
In particular, we will review some of the common errors that can occur when using subproofs.
Consider the following example:
\begin{fitchproof}
	\hypo{a}{A} \pr{}
	\open
		\hypo{b1}{B} \as{for \eif I}
		\have{b2}{B} \by{R}{b1}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
\end{fitchproof}
This is perfectly in keeping with the rules that we have laid down above, and it should not seem particularly surprising.
After all, $B \eif B$ is a tautology, and so follows from no premises.
Thus it is just as easy to derive $B \eif B$ from some starting premises.

But now suppose that tried to continue the proof as follows:
\begin{fitchproof}
	\hypo{a}{A} \pr{}
	\open
		\hypo{b1}{B} \as{for \eif I}
		\have{b2}{B} \by{R}{b1}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
	\have{b}{B} \by{$\eif$E \textsc{(illegal)}}{b2,con}
	%\have [\ ]{x}{} 
\end{fitchproof}
%\by{to invoke $\eif$E}{con, b2}
If we were allowed to do this, we could derive any wfs of $\PL$ from any other.
However, if you tell me that Anne is fast (symbolized by $A$), we shouldn't be able to conclude that Queen Boudica stood twenty-feet tall (symbolized by $B$).
Thankfully we are prohibited from making this move since our rules only permit us to draw on live lines.

Once a subproof closes, the wfss of $\PL$ in that proof are dead, and so we cannot appeal to them individually at a later point in the proof.
This does not mean that we can't appeal to their results, or to the subproof as a whole.
For instance, we could appeal to $B \eif B$ on a later line since this wfs is live throughout the proof. 
Indentation has been included in the proof system to help keep track of what we can and cannot appeal to while writing proofs since it is easy to see which subproofs are closed.
In particular, once you step back one level of indentation, the indented lines of the subproof above are dead, and so can only be cited by certain rules which appeal to the entire subproof, not any one of its lines.

Once we have started thinking about what we can derive from additional assumptions, nothing stops us from asking what we can derive from adding even more assumptions.
Instead of doing this all at once the way that we may begin with many premises, we will do so by opening subproofs within subproofs.
For instance, here is a proof of contraposition:

\begin{proof}
  \hypo{qnp}{Q \eif P} \pr{want $\enot P\eif \enot Q$}  
  \open
    \hypo{a}{\enot P} \as{for $\eif$I}
    \open
      \hypo{q}{Q} \as{for $\enot$I}{}
      \have{np}{P}\ce{qnp,q}
      \have{nnp}{\enot P}\by{R}{a}
    \close
    \have{d}{\enot Q} \ni{q-nnp}
  \close
	\have{nq}{\enot P \eif \enot Q}\ci{a-d}
\end{proof}

Since we can't do anything with a conditional by itself, line $2$ introduces the assumption $\enot P$.
This is a natural choice given that we want to conclude $\enot P \eif \enot Q$.
Even so, there is not much more that we can do than before, and so we are forced to introduce yet another assumption $Q$ on line $3$. 
This is also a natural choice given that we would like to conclude $\enot Q$, and we know that we can use $\enot$I to do so if we reach a contradictory pair of wfss of $\PL$ from assuming $Q$.
Given our assumptions, we may then derive $P$ in line $4$ by appealing to lines $1$ and $3$, both of which are live. 
Since line $2$ is still live, we may derive $\enot P$ on line $5$ by reiteration.
Closing the second subproof, we may justify $\enot Q$ on line $6$ by citing the lines $3$--$5$.
Now can close the first subproof, using these lines to justify $\enot P \eif \enot Q$ on line $7$. 


For contrast, here is a proof where things go awry:
\begin{fitchproof}
\hypo{a}{A} \pr{}
\open
	\hypo{b}{B} \as{for \eif I}
	\open
		\hypo{c}{C} \as{for \eif I}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif(C \eif (A \eand B))}\ci{b-cab}
\have{bcab}{C \eif (A \eand B)}\by{$\eif$I \textsc{(illegal)}}{c-ab}
%\have [\ ]{x}{} \by{to invoke $\eif$I}{c-ab}
\end{fitchproof}
The problem is that the subproof that began with the assumption $C$ was under the assumption of $B$ on line $2$.
By lines $6$ and $7$, we have discharged the assumption $B$, and so are no longer asking what we could show if we assumed $B$.
Although it was perfectly legitimate to draw this same inference on line $5$, by the time we are at line $7$ we cannot appeal to lines $2$--$5$.

Here is one further mistake worth watching out for:
\begin{fitchproof}
\hypo{a}{A} \pr{}
\open
	\hypo{b}{B} \as{for \eif I}
	\open
		\hypo{c}{C} \as{for \eif I}
	\have{bc}{B \eand C}\ai{b,c}
	% \have{c2}{C}\ae{bc}
	\close
\close
  \have{bcab}{B \eif (B \eand C)}\by{$\eif$I (\textsc{illegal})}{b-bc}
%\have [\ ]{x}{} \by{}{b-c2}
\end{fitchproof}
Line $5$ tries to cite a subproof that begins on line $2$ and ends on line $4$, but the wfs on line $4$ depends not only on the assumption on line $2$, but also on another assumption (line $3$) which we have not discharged at the end of the subproof.
Put otherwise, the subproof which starts by assuming $B$ does not end with a wfs at all, but rather ends with a subproof. 
Although we can close both subproofs at once, doing so wouldn't be strategic since line $5$ cannot then cite lines $2$--$4$ to justify $B \eif (B \eand C)$ in the manner presented above.

It is worth further stressing the difference between citing a single line and citing a subproof with a further example.
In particular, when a rule requires you to cite a subproof, you cannot cite an individual line instead, nor \textit{vice versa}.
So for instance, this is incorrect:
\begin{fitchproof}
\hypo{a}{A} \pr{}
\open
	\hypo{b}{B} \as{for \eif I}
	\open
		\hypo{c}{C} \as{for \eif I}
	\have{bc}{B \eand C}\ai{b,c}
	\have{c2}{C}\ae{bc}
	\close
  \have{c3}{C}\by{R (\textsc{illegal})}{c-c2}
%\have [\ ]{x}{} \by{to invoke R}{c-c2}
\close
\have[7]{bcab}{B \eif C}\ci{b-c3}
\end{fitchproof}
Here, we have tried to justify $C$ on line $6$ by the reiteration rule, but we have done so by citing the subproof on lines $3$--$5$.
Although that subproof could in principle be cited on line $6$, the reiteration rule does not permit us to do so.
Rather, we could have used $\eif$I to derive $C \eif C$ while citing that subproof.
By contrast, the reiteration rule R requires you to cite an individual line that is live, so citing the entire subproof is not permissible.

However obvious these mistakes may seem, it can be tempting to bend the rules when writing natural deduction proofs.
This is like bending the rules while playing chess: you simply are no longer playing chess, but rather moving chess pieces around a boards in a manner that is no longer constrained by the rules of chess, or any other game for that matter.
So in writing your own proofs in PL, keep these rules in mind, sticking to them precisely.








\section{Derivability}
  \label{sec:Derivability}

Given the natural deduction rules specified above, we may present the following definition:

\factoidbox{
  A \define{derivation} (or \define{proof}) of $\metaA$ from $\MetaG$ in PL is any finite sequence of wfs of $\PL$ ending in $\metaA$ where every wfs in the sequence is either: (1) a premise in $\MetaG$; (2) an assumption which is eventually discharged; or (3) follows from previous lines by a natural deduction rule for PL besides AS. 
}

A wfs $\metaA$ of $\PL$ is \define{derivable} (or \define{provable}) from $\MetaG$ in PL, i.e., $\MetaG \vdash \metaA$, just in case there is a natural deduction derivation (proof) of $\metaA$ from $\MetaG$ in PL. 
% Whereas `$\MetaG \vdash_{T} \metaA$' was shorthand for `$\MetaG, \enot \metaA \vdash_{T} \bot$', 
Whereas logical consequence provides a semantic answer to the question of what follows from what in virtue of logical form by quantifying over interpretations, the derivation relation $\vdash$ provides a purely syntactic answer to this question by specifying which wfs of $\PL$ can be written after which in a manner which constitutes a derivation.
Perhaps surprisingly, these two relations will be shown to have the same extension, describing formal reasoning in $\PL$ in two different ways.

Having defined the derivation relation for PL, we are now in a position to introduce a number of other definitions.
Two wfss $\metaA$ and $\metaB$ of $\PL$ are \define{interderivable} in PL--- i.e., $\metaA \dashv\vdash \metaB$--- just in case both $\metaA\vdash\metaB$ and $\metaB\vdash\metaA$.
% Additionally, we will take $\MetaG\vdash\bot$ to mean that $\MetaG\vdash\metaB\eand\enot\metaB$ for some $\PL$ sentence $\metaB$.
% It is relatively easy to show that two sentences are provably equivalent since this requires a pair of proofs.
% Showing that sentences are \emph{not} provably equivalent would be much harder.
% It would be just as hard as showing that a sentence is not a theorem.
% (In fact, these problems are interchangeable. Can you think of a sentence that would be a theorem if and only if \metaA{} and \metaB{} were provably equivalent?)
Letting \define{bottom} (also called the \define{falsum}) be the arbitrary contradiction $\bot \colonequals A \wedge \enot A$, we may take a set $\MetaG$ of wfss of $\PL$ to be \define{inconsistent} in PL just in case $\MetaG \vdash \bot$, and \define{consistent} in PL otherwise.

Provability is relative to a proof system.
Whereas the meaning of the `$\vdash$' symbol featured in this chapter concerns PL, later we will use the same symbol for the derivation relation in FOL$^=$, distinguishing these with subscripts as in `$\vdash_{\textsc{pl}}$' and `$\vdash_{\textsc{fol}^=}$' if need be.
% When necessary, we can specify which turnstile we intend by including a reference to the proof system in question, letting `$\vdash_{T}$' stand for provability in the tree system, and `$\vdash_{PL}$' stand for provability in this natural deduction system.
% Since this chapter is concerned with natural deduction, you can take `$\vdash$' to mean `$\vdash_{PL}$' in this chapter unless specified otherwise.
% % For instance, it is important to disambiguate the turnstiles in making the following observation: whereas $\MetaG \vdash_{T} \metaA$ was defined as $\MetaG, \enot \metaA \vdash_{T} \bot$ for the tree proof system, here we have defined $\MetaG \vdash_{PL} \bot$ in terms of $\MetaG \vdash_{PL} \metaB\eand\enot\metaB$ for some $\metaB$. 
% So $\MetaG\vdash\metaA$ can be read as: $\metaA$ is derivable from $\MetaG$.
As with logical consequence, it is often convenient to write `$\metaA_1,\metaA_2,\ldots\vdash\metaB$' as a shorthand for $\set{\metaA_1,\metaA_2,\ldots}\vdash\metaB$.
A wfs $\metaA$ of $\PL$ is a \define{theorem} of PL just in case $\vdash\metaA$.
As with the derivations in PL, it is important to note that the wfss of $\PL$ are only theorems relative to a proof system, and so there is no sense in which $A\eor\enot A$ is a theorem full stop.
Nevertheless, it is natural to expect $A\eor\enot A$ to be a theorem of most any proof system for $\PL$.

In order to show that something is a theorem we have to derive it from no premises.
But how could we show that something is \emph{not} a theorem?
More generally, how could we show that $\MetaG \nproves \metaA$?
Showing that there is no proof of $\metaA$ from $\MetaG$ would seem to require searching the space of all natural deduction proofs, and this is not bound to be easy. 
For instance, even if you (or a computer program) failed to derive $\metaA$ from $\MetaG$ in a thousand different ways, perhaps their is a proof that has not yet been considered.
This brings out an important difference between our natural deduction system PL and the truth table method presented above for deciding whether $\MetaG \vDash \metaA$.
In particular, PL does not provide an effective procedure for determining whether $\MetaG \vDash \metaA$.
However, consider the following:

\factoidbox{
  \textsc{PL Soundness:} If $\MetaG \vdash \metaA$, then $\MetaG \vDash \metaA$.
}

Given the above, we may show that $\MetaG \nvdash \metaA$ by proving that $\MetaG \nvDash \metaA$ by finding an interpretation $\I$ of $\PL$ in which $\V{\I}(\metaG) = 1$ for all $\metaG \in \MetaG$ but $\V{\I}(\metaA) = 0$. 
The reasoning goes by contraposition.
After all, if $\MetaG \vdash \metaA$, then $\MetaG \vDash \metaA$ follows by \textsc{Soundness}, and so $\MetaG \vdash \metaA$ cannot be the case so long as we have shown that $\MetaG \nvDash \metaA$.
Thus we may conclude that $\MetaG \nvdash \metaA$.

Suppose instead that we want to show that $\MetaG \vdash \metaA$ but cannot seem to find a proof however hard we look. 
Here is another important principle to which we might appeal:

\factoidbox{
  \textsc{PL Completeness:} If $\MetaG \vDash \metaA$, then $\MetaG \vdash \metaA$.
}

If we can show that $\MetaG \vDash \metaA$ either by constructing a truth table or writing a semantic proof, \textsc{Completeness} permits us to conclude that $\MetaG \vdash \metaA$, and so we know that there is a derivation of $\metaA$ from $\MetaG$ even if we haven't managed to find one.
This is different from knowing \textit{how} to derive $\metaA$ from $\MetaG$, but valuable information nonetheless. 
In order to make use of the principles above, the following two chapters will establish the soundness and completeness of PL.


% It is easy to show that a set is provably inconsistent: You just need to assume the sentences in the set and prove a contradiction. Showing that a set is \emph{not} provably inconsistent will be much harder. It would require more than just providing a proof or two; it would require showing that proofs of a certain kind are \emph{impossible}.





\iffalse

\subsection{Replacement}

Consider how you would prove this argument form valid: $F\eif(G\eand H)$ \therefore\ $F\eif G$

Perhaps it is tempting to write down the premise and apply the $\eand$E rule to the conjunction $(G \eand H)$. This is impermissible, however, because the basic rules of proof can only be applied to whole sentences. In order to use $\eand$E, we need to get the conjunction $(G \eand H)$ on a line by itself. Here is a proof:

\begin{proof}
	\hypo{fgh}{F\eif(G\eand H)} \pr{}
	\open
		\hypo{f}{F}\as{want $G$}
		\have{gh}{G \eand H}\ce{fgh,f}
		\have{g}{G}\ae{gh}
	\close
	\have{fg}{F \eif G}\ci{f-g}
\end{proof}

The rules we have seen so far must apply to wffs that are on a proof line by themselves. We will now introduce some derived rules that may be applied to wffs that are parts of more complex sentences. These are called \define{rules of replacement}, because they can be used to replace part of a sentence with a logically equivalent expression. One simple rule of replacement is Commutativity (abbreviated Comm), which says that we can swap the order of conjuncts in a conjunction or the order of disjuncts in a disjunction. We define the rule thus:

\begin{center}
\begin{tabular}{rl}
$(\metaA{}\eand\metaB{}) \Longleftrightarrow (\metaB{}\eand\metaA{})$\\
$(\metaA{}\eor\metaB{}) \Longleftrightarrow (\metaB{}\eor\metaA{})$\\
$(\metaA{}\eiff\metaB{}) \Longleftrightarrow (\metaB{}\eiff\metaA{})$
& Comm
\end{tabular}
\end{center}

The double arrow means that you can take a subformula on one side of the arrow and replace it with the subformula on the other side. The arrow is double-headed because rules of replacement work in both directions. And replacement rules --- unlike all the rules we've seen so far --- can be applied to wffs that are part of more complex sentences. They don't need to be on their own line.

Consider this argument: $(M \eor P) \eif (P \eand M)$ \therefore\ $(P \eor M) \eif (M \eand P)$

It is possible to give a proof of this using only the basic rules, but it will be somewhat tedious. With the Comm rule, we can provide a proof easily:

\begin{proof}
	\hypo{1}{(M \eor P) \eif (P \eand M)} \pr{}
	\have{2}{(P \eor M) \eif (P \eand M)}\by{Comm}{1}
	\have{n}{(P \eor M) \eif (M \eand P)}\by{Comm}{2}
\end{proof}

(We need to apply the rule twice, because each application allows one transformation. We transformed the antecedent first, then the consequent. The opposite order would also have been fine.)

Another rule of replacement is Double Negation (DN). With the DN rule, you can remove or insert a pair of negations for any wff in a line, even if it isn't the whole line. This is the rule:

\begin{center}
\begin{tabular}{rl}
$\enot\enot\metaA{} \Longleftrightarrow \metaA{}$ & DN
\end{tabular}
\end{center}

Two more replacement rules  are called De Morgan's Laws, named for the 19th-century British logician August De Morgan. (Although De Morgan did formalize and publish these laws, many others discussed them before him.) The rules capture useful relations between negation, conjunction, and disjunction. Here are the rules, which we abbreviate DeM:

\begin{center}
\begin{tabular}{rl}
$\enot(\metaA{}\eor\metaB{}) \Longleftrightarrow (\enot\metaA{}\eand\enot\metaB{})$\\
$\enot(\metaA{}\eand\metaB{}) \Longleftrightarrow (\enot\metaA{}\eor\enot\metaB{})$
& DeM
\end{tabular}
\end{center}

As we have seen, $\metaA{}\eif\metaB{}$ is equivalent to $\enot\metaA{}\eor\metaB{}$. A further replacement rule captures this equivalence. We abbreviate the rule MC, for `material conditional.' It takes two forms:

\begin{center}
\begin{tabular}{rl}
$(\metaA{}\eif\metaB{}) \Longleftrightarrow (\enot\metaA{}\eor\metaB{})$ &\\
$(\metaA{}\eor\metaB{}) \Longleftrightarrow (\enot\metaA{}\eif\metaB{})$ & MC
\end{tabular}
\end{center}

Now consider this argument: $\enot(P \eif Q)$ \therefore\ $P \eand \enot Q$

As always, we could prove this argument valid using only the basic rules. With rules of replacement, though, the proof is much simpler:

\begin{proof}
	\hypo{1}{\enot(P \eif Q)} \pr{}
	\have{2}{\enot(\enot P \eor Q)}\by{MC}{1}
	\have{3}{\enot\enot P \eand \enot Q}\by{DeM}{2}
	\have{4}{P \eand \enot Q}\by{DN}{3}
\end{proof}

A final replacement rule captures the relation between conditionals and biconditionals. We will call this rule biconditional exchange and abbreviate it $\eiff${ex}.

\begin{center}
\begin{tabular}{rl}
$[(\metaA{}\eif\metaB{})\eand(\metaB{}\eif\metaA{})] \Longleftrightarrow (\metaA{}\eiff\metaB{})$
& $\eiff${ex}
\end{tabular}
\end{center}



%
%Although they don't do it in the book, I've been in the habit of writing $(\metaA{}\eand\metaB{}\eand\metaC{})$ and dropping the inner pair of parentheses. This is fine. If we'd wanted to, we could have defined the basic rules in a more general way:
%
%\begin{proof}
%	\have[n]{a1}{\metaA{}_1}
%	\have{2}{\metaA{}_2}
%	\have[\vdots]{1}{\vdots}
%	\have[n]{an}{\metaA{}_n}
%	\have[\ ]{aaa}{\metaA{}_1~\eand\ldots\eand~\metaA{}_n} \ai{}
%\end{proof}
%
%\bigskip
%\begin{proof}
%	\have{3}{\metaA{}_1~\eand\ldots\eand~\metaA{}_n}
%	\have{1}{\metaA{}_i} \ae{}
%\end{proof}
%
%\bigskip
%\begin{proof}
%	\have{1}\metaA{}
%	\have{3}{\metaA{}\eor\metaB{}_1\eor\metaB{}_2\ldots\eor\metaB{}_n} \ai{}
%\end{proof}
%
%We don't need these extended versions, since for any given n we could prove them as a derived rule.
%
%
%The basic rules for conjunction can be valuable in a proof even if there are no conjunctions in any of the assumptions; the basic rules for disjunction can be used even if there are no disjunctions in any assumptions; and similarly for the other basic rules. The rules for identity are different, in that there must be an identity claim in some assumption in order for the rules to do any work. Other than the trivial identity that we can introduce with the {=}I rule
%
%
%do not apply we can now prove that identity is \emph{transitive}: If $a=b$ and $b=c$, then $a=c$. The proof proceeds in this way:
%\begin{proof}
%	\open
%		\hypo{p}{a=b \eand b=c}\by{want $a=c$}{}
%		\have{ab}{a=b}\ae{p}
%		\have{bc}{b=c}\ae{p}
%		\have{ac}{a=c}\by{{=}E}{ab,bc}
%	\close
%	\have{conc}{(a=b \eand b=c)\eif a=c} \ci{p-ac}
%\end{proof}
%
%
%As an example, consider this argument:
%\begin{quote}
%There is only one button in my pocket. There is a blue button in my pocket. Therefore, there is no button in my pocket that is not blue.
%\end{quote}
%We begin by defining a symbolization key:
%\begin{ekey}
%\item{UD:} buttons in my pocket
%\item{Bx:} $x$ is blue.
%\end{ekey}
%\begin{proof}
%	\hypo{one}{\forall x\forall y\ x=y}
%	\hypo{eb}{\exists x Bx} \by{want $\enot\exists x \enot Bx$}{}
%	\open
%		\hypo{be1}{Be}
%		\have{ef1}{e=f}\Ae{one}
%		\have{bf1}{Bf}\by{{=}E}{ef1,be1}
%	\close
%	\have{bf}{Bf}\Ee{eb,be1-bf1}
%	\have{ab}{\forall x Bx}\Ai{bf}
%	\have{nnab}{\enot\enot\forall x Bx}\by{DN}{ab}
%	\have{nenb}{\enot\exists x\enot Bx}\by{QN}{nnab}
%\end{proof}

% \iffalse
 
\clearpage



\practiceproblems

\solutions
\problempart
\label{pr.justifyPLproof}
Provide a justification (rule and line numbers) for each line of proof that requires one {\color{black}(which for \textit{Carnap} means ALL of them!)}
\begin{multicols}{2}
\begin{proof}
\hypo{1}{W \eif \enot B}
\hypo{2}{A \eand W}
\hypo{2b}{B \eor (J \eand K)}
\have{3}{W}{}
\have{4}{\enot B} {}
\have{5}{J \eand K} {}
\have{6}{K}{}
\end{proof}

\begin{proof}
\hypo{1}{L \eiff \enot O}
\hypo{2}{L \eor \enot O}
\open
	\hypo{a1}{\enot L}
	\have{a2}{\enot O}{}
	\have{a3}{L}{}
	\have{a4}{\enot L}{}
\close
\have{3}{L}{}
\end{proof}

\begin{proof}
\hypo{1}{Z \eif (C \eand \enot N)}
\hypo{2}{\enot Z \eif (N \eand \enot C)}
\open
	\hypo{a1}{\enot(N \eor  C)}
	\have{a2}{\enot N \eand \enot C} {}
	\open
		\hypo{b1}{Z}
		\have{b2}{C \eand \enot N}{}
		\have{b3}{C}{}
		\have{b4}{\enot C}{}
	\close
	\have{a3}{\enot Z}{}
	\have{a4}{N \eand \enot C}{}
	\have{a5}{N}{}
	\have{a6}{\enot N}{}
\close
\have{3}{N \eor C}{}
\end{proof}
\end{multicols}

\solutions
\problempart
\label{pr.solvedPLproofs}
Give a proof for each argument in $\PL$.
\begin{earg}
\item $K\eand L$, \therefore $K\eiff L$
\item $A\eif (B\eif C)$, \therefore $(A\eand B)\eif C$
\item $P \eand (Q\eor R)$, $P\eif \enot R$, \therefore $Q\eor E$
\item $(C\eand D)\eor E$, \therefore $E\eor D$
\item $\enot F\eif G$, $F\eif H$, \therefore $G\eor H$
\item $(X\eand Y)\eor(X\eand Z)$, $\enot(X\eand D)$, $D\eor M$ \therefore $M$
\end{earg}

\problempart
Give a proof for each argument in $\PL$.
\begin{earg}
\item $Q\eif(Q\eand\enot Q)$, \therefore\ $\enot Q$
\item $J\eif\enot J$, \therefore\ $\enot J$
\item $E\eor F$, $F\eor G$, $\enot F$, \therefore\ $E \eand G$
\item $A\eiff B$, $B\eiff C$, \therefore\ $A\eiff C$
\item $M\eor(N\eif M)$, \therefore\ $\enot M \eif \enot N$
\item $S\eiff T$, \therefore\ $S\eiff (T\eor S)$
\item $(M \eor N) \eand (O \eor P)$, $N \eif P$, $\enot P$, \therefore\ $M\eand O$
\item $(Z\eand K) \eor (K\eand M)$, $K \eif D$, \therefore\ $D$
\end{earg}


\solutions
\problempart
\label{pr.PLND.theorems}
Show that each of the following sentences is a theorem in $\PL$.
\begin{earg}
\item $O \eif O$
\item $N \eor \enot N$
\item $\enot(P\eand \enot P)$
\item $\enot(A \eif \enot C) \eif (A \eif C)$
\item $J \eiff [J\eor (L\eand\enot L)]$
\end{earg}

\problempart
Show that each of the following pairs of sentences are provably equivalent in $\PL$.
\begin{earg}
\item $\enot\enot\enot\enot G$, $G$
\item $T\eif S$, $\enot S \eif \enot T$
\item $R \eiff E$, $E \eiff R$
\item $\enot G \eiff H$, $\enot(G \eiff H)$
\item $U \eif I$, $\enot(U \eand \enot I)$
\end{earg}

\solutions
\problempart
\label{pr.solvedPLproofs2}
Provide proofs to show each of the following.
\begin{earg}
\item $M \eand (\enot N \eif \enot M) \vdash (N \eand M) \eor \enot M$
\item \{$C\eif(E\eand G)$, $\enot C \eif G$\} $\vdash$ $G$
\item \{$(Z\eand K)\eiff(Y\eand M)$, $D\eand(D\eif M)$\} $\vdash$ $Y\eif Z$
\item \{$(W \eor X) \eor (Y \eor Z)$, $X\eif Y$, $\enot Z$\} $\vdash$ $W\eor Y$
\end{earg}



\problempart
For the following, provide proofs using only the basic rules. The proofs will be longer than proofs of the same claims would be using the derived rules.
\begin{earg}
\item Show that MT is a legitimate derived rule. Using only the basic rules, prove the following: \metaA{}\eif\metaB{}, \enot\metaB{}, \therefore\ \enot\metaA{}
\item Show that Comm is a legitimate rule for the biconditional. Using only the basic rules, prove that $\metaA{}\eiff\metaB{}$ and $\metaB{}\eiff\metaA{}$ are equivalent.
\item Using only the basic rules, prove the following instance of DeMorgan's Laws: $(\enot A \eand \enot B)$, \therefore\ $\enot(A \eor B)$
\item Show that $\eiff${ex} is a legitimate derived rule. Using only the basic rules, prove that $D\eiff E$ and $(D\eif E)\eand(E\eif D)$ are equivalent.
\end{earg}




\problempart
\begin{earg}
\item If you know that $\metaA{}\vdash\metaB{}$, what can you say about $(\metaA{}\eand\metaC{})\vdash\metaB{}$? Explain your answer.
\item If you know that $\metaA{}\vdash\metaB{}$, what can you say about $(\metaA{}\eor\metaC{})\vdash\metaB{}$? Explain your answer.
\end{earg}



%****************STUFF JH CUT FROM UBC EDITION*******************************


% \iffalse 

\section{Ichikawa: The remaining basic rules}

All of the rules introduced in this chapter are summarized in the Quick Reference guide at the end of this book.

\subsection{Disjunction Introduction}
If $M$ is true, then $M \eor N$ must also be true. In general, the Disjunction Introduction rule ($\eor$I) allows us to derive a disjunction if we already have one of its two disjuncts:

\begin{proof}
	\have[m]{a}\metaA{}
	\have[\ ]{ab}{\metaA{}\eor\metaB{}}\oi{a}
	\have[\ ]{ba}{\metaB{}\eor\metaA{}}\oi{a}
\end{proof}

One can introduce a disjunct in either position --- it can be the first disjunct or the second disjunct. Accordingly, both options are listed here. (One is not required to do both; you can just take whichever version you find most helpful.)

As always, \metaB{} can be \emph{any} sentence whatsoever. So the following is a legitimate proof:

\begin{proof}
	\hypo{m}{M}
	\have{mmm}{M \eor ([(A\eiff B) \eif (C \eand D)] \eiff [E \eand F])}\oi{m}
\end{proof}

It may seem odd that just by knowing $M$ we can derive a conclusion that includes sentences like $A$, $B$, and the rest --- sentences that have nothing to do with $M$. Yet the conclusion follows immediately by $\eor$I. This is as it should be: The truth conditions for the disjunction mean that, if \metaA{} is true, then $\metaA{}\eor \metaB{}$ is true regardless of what \metaB{} is. So the conclusion could not be false if the premise were true; the argument form is valid.



\subsection{Conditional Introduction}

Consider this argument:
\begin{earg}
\item[] $R \eor F$
\item[\therefore] $\enot R \eif F$
\end{earg}
The argument form seems like it should be valid --- either $R$ or $F$ is true, so if $R$ isn't true, $F$ must be. (You can confirm that it is valid by examining the truth tables.) The Conditional Introduction rule can demonstrate that this is so.

We begin the proof by writing down the premise of the argument, making a note of our intended conclusion, and drawing a horizontal line, like this:

\begin{proof}
	\hypo{rf}{R \eor F} \want{\enot R \eif F}
\end{proof}

If we had $\enot R$ as a further premise, we could derive $F$ by the $\eor$E rule. But we do not have $\enot R$ as a premise of this argument, nor can we derive it directly from the premise we do have --- so we cannot simply prove $F$. What we will do instead is start a \emph{subproof}, a proof within the main proof. When we start a subproof, we draw another vertical line to indicate that we are no longer in the main proof. Then we write in an assumption for the subproof. This can be anything we want. Here, it will be helpful to assume $\enot R$. We want to show that, if we did assume that, we would be able to derive $F$. So we make a new assumption that $\enot R$, and give ourselves a note that we wish to derive $F$. Our proof now looks like this:

\begin{proof}
	\hypo{rf}{R \eor F}  \want{\enot R \eif F}
	\open
		\hypo{nr}{\enot R} \want {F}
		\have{}{}
	\close
\end{proof}

It is important to emphasize that we are not claiming to have \emph{proven} $\enot R$ from the premise on line 1. We do not need to write in any justification for the assumption line of a subproof. (The `want' is a note to ourself, not a justification.) The new horizontal line indicates that a new \emph{assumption} is being made; we also include a vertical line that extend to future lines, to indicate that this assumption remains in effect. You can think of the subproof as posing the question: What could we show \emph{if} $\enot R$ were true? We are trying to show that we could derive $F$. And indeed, we can:

\begin{proof}
	\hypo{rf}{R \eor F}  \want{\enot R \eif F}
	\open
		\hypo{nr}{\enot R} \want {F}
		\have{f}{F}\by{DS}{rf, nr}
	\close
\end{proof}

This has shown that \emph{if} we had $\enot R$ as a premise, \emph{then} we could prove $F$. In effect, we have proven that $F$ follows from $\enot R$. This is indeed very close to demonstrating the conditional, $\enot R \eif F$. This last step is just what the Conditional Introduction rule will allow us to perform. We close the subproof and derive $\enot R \eif F$ in the main proof. Our final proof looks like this:

\begin{proof}
	\hypo{rf}{R \eor F}
	\open
		\hypo{nr}{\enot R}\want {F}
		\have{f}{F}\by{DS}{rf, nr}
	\close
	\have{nrf}{\enot R \eif F}\ci{nr-f}
\end{proof}

The $\eif$I rule lets us \define{discharge} the assumption we'd been making, ending that vertical line. We also stop indenting --- the difference in placement of lines 3 and 4 emphasizes that they are importantly different: during lines 2 and 3, we were \emph{assuming} that \enot $R$. By the time we get to line 4, we are no longer making that assumption.

Notice that the justification for applying the $\eif$I rule is the entire subproof. That's why we justify it by reference to a range of lines, instead of a comma-separated list. Usually that will be more than just two lines.

It may seem as if the ability to assume anything at all in a subproof would lead to chaos: Does it allow you to prove any conclusion from any premises? The answer is no, it does not. Consider this proof schema:

\begin{proof}
	\hypo{a}\metaA{}
	\open
		\hypo{b1}\metaB{}
		\have{b2}{\metaB{}\eand \metaA{}} \ai{a,b1}
	\close
\end{proof}

Does this show that one can conjoin any arbitrary sentence \metaB{} with premise \metaA{}? After all, we've written \metaB{}\eand\metaA{} on a line of a proof that began with \metaA{}, without violating any of the rules of our system. The reason this doesn't have that implication is the vertical line that still extends into line 3. That line indicates that the assumption made at line 2 is still in effect. When the vertical line for the subproof ends, the subproof is \emph{closed}. In order to complete a proof, you must close all of the subproofs. The conclusion to be proved must not be `blocked off' by a vertical line; it should be aligned with the premises.

In this example, there is no way to close the subproof and show that the conjunction follows from line 1 alone. One can only close a subproof via particular rules that allow you to do so; $\eif$I is one such rule; $\eand$I does not close subproofs. One can't just close a subproof willy-nilly. Closing a subproof is called \emph{discharging} the assumptions of that subproof. So we can put the point this way: You cannot complete a proof until you have discharged all of the assumptions other than the original premises of the argument.

Of course, it is legitimate to do this:

\begin{proof}
	\hypo{a}\metaA{}
	\open
		\hypo{b1}\metaB{}
		\have{b2}{\metaB{}\eand \metaA{}} \ai{a,b1}
	\close
	\have{c}{\metaB{} \eif (\metaB{}\eand\metaA{})} \ci{b1-b2}
\end{proof}

This should not seem so strange, though. The conclusion on line 4 really does follow from line 1. (Draw a truth table if you need convincing of this.) 

Once an assumption has been discharged, any lines that have been shown to follow from that assumption --- i.e., those lines inside the box indicated by the vertical line of that assumption --- cannot be cited as justification on further lines. So this development of the proof above, for instance, is not permitted:

\begin{proof}
	\hypo{rf}{R \eor F}
	\open
		\hypo{nr}{\enot R}\want {F}
		\have{f}{F}\by{DS}{rf, nr}
	\close
	\have{nrf}{\enot R \eif F}\ci{nr-f}
	\have{bad}{F \eor A}\oi{f}
\end{proof}

Once the assumption made at line 2 has been discharged at line 4, the lines within that assumption --- 2 and 3 --- are unavailable for further justification. So one cannot perform Disjunctive Syllogism  on line 3 at line 5. Line 3 was not demonstrated to follow from the premise on line 1 --- it follows only from this combined with the \emph{assumption} on line 2. And by the time we get to line 5, we are no longer making that assumption.

Put in its general form, the $\eif$I rule looks like this:

\begin{proof}
	\open
		\hypo[m]{a}\metaA{} \by{want \metaB{}}{}
		\have[n]{b}\metaB{}
	\close
	\have[\ ]{ab}{\metaA{}\eif\metaB{}}\ci{a-b}
\end{proof}

When we introduce a subproof, we typically write what we want to derive off to the right. This is just so that we do not forget why we started the subproof if it goes on for five or ten lines. There is no `want' rule. It is a note to ourselves, and not formally part of the proof.

Although it is consistent with the natural deduction rules to open a subproof with any assumption you please, there is some strategy involved in picking a useful assumption. Starting a subproof with a random assumption is a terrible strategy. It will just waste lines of the proof. In order to derive a conditional by the $\eif$I rule, for instance, you must assume the antecedent of that conditional.

The $\eif$I rule also requires that the consequent of the conditional be the last line of the subproof. It is always permissible to close a subproof and discharge its assumptions, but it will not be helpful to do so until you get what you want. This is an illustration of the observation made above, that unlike the tree method, the natural deduction method requires some strategy and thinking ahead.

You should \emph{never} make an assumption without a plan for how to discharge it.





\subsection{Biconditional Introduction}

Biconditionals indicate that the two sides have the same truth value. One establishes a biconditional by establishing each direction of it as conditionals. To derive $W \eiff X$, for instance, you must establish both $W \eif X$ and $X \eif W$. (You might derive those conditionals via $\eif$I, or you might get them some other way. They might even simply be premises.) Those conditionals may occur in either order; they need not be on consecutive lines. (Compare the shape of the $\eand$I rule.) Schematically, the Biconditional Introduction rule works like this:

\begin{proof}
	\have[m]{ab}{\metaA{}\eif\metaB{}}
	\have[n]{ba}{\metaB{}\eif\metaA{}}
	\have[\ ]{c}{\metaA{}\eiff\metaB{}} \bi{ab, ba}
\end{proof}


\subsection{Biconditional Elimination}

The Biconditional Elimination rule ($\eiff$E) is a generalized version of \emph{modus ponens} ($\eif$E). If you have the left-hand subsentence of the biconditional, you can derive the right-hand subsentence. If you have the right-hand subsentence, you can derive the left-hand subsentence. This is the rule:



\begin{multicols}{2}
\begin{proof}
	\have[m]{ab}{\metaA{}\eiff\metaB{}}
	\have[n]{a}\metaA{}
	\have[\ ]{b}\metaB{} \be{ab,a}
\end{proof}
\begin{proof}
	\have[m]{ab}{\metaA{}\eiff\metaB{}}
	\have[n]{a}\metaB{}
	\have[\ ]{b}\metaA{} \be{ab,a}
\end{proof}
\end{multicols}

As in the case of Disjunction Elimination, we include both versions under the same name, so that you don't need to worry about whether the side you already have is the left-hand side of the biconditional or the right-hand side. Whichever side it is, you may derive the other via Biconditional Elimination.

\fi 

